{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package & Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/Code/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hao/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hao/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza.download('en')\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_locID</th>\n",
       "      <th>Hotel_geoID_x</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Date_of_stay</th>\n",
       "      <th>Reviewer_Contributions</th>\n",
       "      <th>Reviewer_helpful_vote</th>\n",
       "      <th>Review_Rating</th>\n",
       "      <th>Trip_type</th>\n",
       "      <th>Review_helpful_votes</th>\n",
       "      <th>...</th>\n",
       "      <th>Resaurant_count</th>\n",
       "      <th>Attractions_count</th>\n",
       "      <th>Hotel_styles</th>\n",
       "      <th>Popular_mentions</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Unreliable</th>\n",
       "      <th>Stanza_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>916928494</td>\n",
       "      <td>Sep 16</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>66</td>\n",
       "      <td>Unfriendly staff and dirty</td>\n",
       "      <td>I travel a lot - and I am in general very flex...</td>\n",
       "      <td>-0.4095</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>828890910</td>\n",
       "      <td>Feb 2022</td>\n",
       "      <td>February 2022</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>115</td>\n",
       "      <td>Perfect for Us</td>\n",
       "      <td>We recently chose Motto for an overnight in NY...</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>915189618</td>\n",
       "      <td>Sep 6</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>99</td>\n",
       "      <td>Not satisfied with the overall experience</td>\n",
       "      <td>The rooms are two small for the price that you...</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>915010751</td>\n",
       "      <td>Sep 5</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>68</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>152</td>\n",
       "      <td>Magnificent Motto - fantastic staff</td>\n",
       "      <td>I have just returned home after a five day sta...</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>914794870</td>\n",
       "      <td>Sep 4</td>\n",
       "      <td>August 2023</td>\n",
       "      <td>179</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>201</td>\n",
       "      <td>Worthy of a 5-star rating!</td>\n",
       "      <td>I did not believe the overwhelming 5-star revi...</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65977</th>\n",
       "      <td>12841013</td>\n",
       "      <td>35805</td>\n",
       "      <td>536016764</td>\n",
       "      <td>Oct 2017</td>\n",
       "      <td>October 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>209.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>107</td>\n",
       "      <td>Great cost benefit</td>\n",
       "      <td>I stayed in the loop suites from 10/19/17 to 1...</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65978</th>\n",
       "      <td>12841013</td>\n",
       "      <td>35805</td>\n",
       "      <td>535664344</td>\n",
       "      <td>Oct 2017</td>\n",
       "      <td>October 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>209.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>43</td>\n",
       "      <td>AWESOME</td>\n",
       "      <td>I enjoy my stay! Very nice and very clean and ...</td>\n",
       "      <td>0.9703</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65979</th>\n",
       "      <td>12841013</td>\n",
       "      <td>35805</td>\n",
       "      <td>535094403</td>\n",
       "      <td>Oct 2017</td>\n",
       "      <td>January 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>209.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>46</td>\n",
       "      <td>Very good hotel, highly recommended</td>\n",
       "      <td>I've stayed in this apartment hotel in January...</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65980</th>\n",
       "      <td>12841013</td>\n",
       "      <td>35805</td>\n",
       "      <td>531949578</td>\n",
       "      <td>Oct 2017</td>\n",
       "      <td>May 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>209.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>158</td>\n",
       "      <td>Shocking</td>\n",
       "      <td>We booked an apartment for 4 people, 2 couples...</td>\n",
       "      <td>-0.5707</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65981</th>\n",
       "      <td>12841013</td>\n",
       "      <td>35805</td>\n",
       "      <td>531808345</td>\n",
       "      <td>Oct 2017</td>\n",
       "      <td>October 2017</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled with friends</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>209.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>62</td>\n",
       "      <td>Horrible!!!</td>\n",
       "      <td>I'm sorry for my English, but I write why ever...</td>\n",
       "      <td>0.2937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65982 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hotel_locID  Hotel_geoID_x  Review_id Review_Date    Date_of_stay  \\\n",
       "0         23448880          60763  916928494      Sep 16  September 2023   \n",
       "1         23448880          60763  828890910    Feb 2022   February 2022   \n",
       "2         23448880          60763  915189618       Sep 6  September 2023   \n",
       "3         23448880          60763  915010751       Sep 5  September 2023   \n",
       "4         23448880          60763  914794870       Sep 4     August 2023   \n",
       "...            ...            ...        ...         ...             ...   \n",
       "65977     12841013          35805  536016764    Oct 2017    October 2017   \n",
       "65978     12841013          35805  535664344    Oct 2017    October 2017   \n",
       "65979     12841013          35805  535094403    Oct 2017    January 2017   \n",
       "65980     12841013          35805  531949578    Oct 2017        May 2017   \n",
       "65981     12841013          35805  531808345    Oct 2017    October 2017   \n",
       "\n",
       "       Reviewer_Contributions  Reviewer_helpful_vote  Review_Rating  \\\n",
       "0                           0                      0              1   \n",
       "1                          45                     26              5   \n",
       "2                           0                      0              3   \n",
       "3                          68                     88              5   \n",
       "4                         179                     68              5   \n",
       "...                       ...                    ...            ...   \n",
       "65977                       0                      0              4   \n",
       "65978                       0                      0              5   \n",
       "65979                       0                      0              5   \n",
       "65980                       0                      0              1   \n",
       "65981                      40                     31              1   \n",
       "\n",
       "                   Trip_type  Review_helpful_votes  ...  Resaurant_count  \\\n",
       "0       Traveled on business                     1  ...            348.0   \n",
       "1                        NaN                     8  ...            348.0   \n",
       "2       Traveled on business                     0  ...            348.0   \n",
       "3       Traveled with family                     1  ...            348.0   \n",
       "4              Traveled solo                     0  ...            348.0   \n",
       "...                      ...                   ...  ...              ...   \n",
       "65977          Traveled solo                     0  ...            209.0   \n",
       "65978          Traveled solo                     0  ...            209.0   \n",
       "65979          Traveled solo                     0  ...            209.0   \n",
       "65980   Traveled as a couple                     0  ...            209.0   \n",
       "65981  Traveled with friends                     0  ...            209.0   \n",
       "\n",
       "       Attractions_count  Hotel_styles  \\\n",
       "0                  100.0  ['Business']   \n",
       "1                  100.0  ['Business']   \n",
       "2                  100.0  ['Business']   \n",
       "3                  100.0  ['Business']   \n",
       "4                  100.0  ['Business']   \n",
       "...                  ...           ...   \n",
       "65977               90.0            []   \n",
       "65978               90.0            []   \n",
       "65979               90.0            []   \n",
       "65980               90.0            []   \n",
       "65981               90.0            []   \n",
       "\n",
       "                                        Popular_mentions  WordCount  \\\n",
       "0      ['front desk', 'landmark view', 'city view roo...         66   \n",
       "1      ['front desk', 'landmark view', 'city view roo...        115   \n",
       "2      ['front desk', 'landmark view', 'city view roo...         99   \n",
       "3      ['front desk', 'landmark view', 'city view roo...        152   \n",
       "4      ['front desk', 'landmark view', 'city view roo...        201   \n",
       "...                                                  ...        ...   \n",
       "65977                                                 []        107   \n",
       "65978                                                 []         43   \n",
       "65979                                                 []         46   \n",
       "65980                                                 []        158   \n",
       "65981                                                 []         62   \n",
       "\n",
       "                                           Title  \\\n",
       "0                     Unfriendly staff and dirty   \n",
       "1                                 Perfect for Us   \n",
       "2      Not satisfied with the overall experience   \n",
       "3            Magnificent Motto - fantastic staff   \n",
       "4                     Worthy of a 5-star rating!   \n",
       "...                                          ...   \n",
       "65977                         Great cost benefit   \n",
       "65978                                    AWESOME   \n",
       "65979        Very good hotel, highly recommended   \n",
       "65980                                   Shocking   \n",
       "65981                                Horrible!!!   \n",
       "\n",
       "                                                  Review Compound_Score  \\\n",
       "0      I travel a lot - and I am in general very flex...        -0.4095   \n",
       "1      We recently chose Motto for an overnight in NY...         0.9903   \n",
       "2      The rooms are two small for the price that you...        -0.0121   \n",
       "3      I have just returned home after a five day sta...         0.9746   \n",
       "4      I did not believe the overwhelming 5-star revi...         0.9949   \n",
       "...                                                  ...            ...   \n",
       "65977  I stayed in the loop suites from 10/19/17 to 1...         0.9891   \n",
       "65978  I enjoy my stay! Very nice and very clean and ...         0.9703   \n",
       "65979  I've stayed in this apartment hotel in January...         0.7639   \n",
       "65980  We booked an apartment for 4 people, 2 couples...        -0.5707   \n",
       "65981  I'm sorry for my English, but I write why ever...         0.2937   \n",
       "\n",
       "       Unreliable Stanza_Score  \n",
       "0               0            0  \n",
       "1               0            2  \n",
       "2               0            0  \n",
       "3               0            2  \n",
       "4               0            2  \n",
       "...           ...          ...  \n",
       "65977           0            2  \n",
       "65978           0            2  \n",
       "65979           0            2  \n",
       "65980           0            0  \n",
       "65981           0            0  \n",
       "\n",
       "[65982 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Final_Datasets/TA_combined_df_City_tourism_type_VADER_final_Stanza.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Hotel_locID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview & Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER\n",
    "def calculate_compound_score(review):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(review)['compound']\n",
    "\n",
    "# Unreliable tag\n",
    "def calculate_unreliable(row):\n",
    "    compound_score = row['Compound_Score']\n",
    "    rating = row['Review_Rating']\n",
    "    \n",
    "    if (compound_score < -0.49 and rating >= 3):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Stanza\n",
    "# def analyze_sentiment(text):\n",
    "#   doc = nlp(text)\n",
    "#   sentiments = [sentence.sentiment for sentence in doc.sentences]\n",
    "  \n",
    "#   compound_score = sentiments[0]\n",
    "#   return compound_score\n",
    "\n",
    "# Stanza x iterations\n",
    "def analyze_sentiment(text):\n",
    "    compound_scores = []\n",
    "    num_iterations=11\n",
    "    for _ in range(num_iterations):\n",
    "        doc = nlp(text)\n",
    "        sentiments = [sentence.sentiment for sentence in doc.sentences]\n",
    "        compound_scores.append(sentiments[0])\n",
    "    \n",
    "    most_common_sentiment = Counter(compound_scores).most_common(1)\n",
    "    # print(compound_scores)\n",
    "    \n",
    "    return most_common_sentiment[0][0]\n",
    "\n",
    "# Stanza Unreliable tag\n",
    "def calculate_unreliable_stanza(row):\n",
    "    stanza_score = row['Stanza_Score']\n",
    "    rating = row['Review_Rating']\n",
    "    \n",
    "    if (stanza_score == 2 and rating < 3) or (stanza_score == 0 and rating >= 3):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unreliable\n",
      "0    64966\n",
      "1     1016\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Unreliable'] = df.apply(calculate_unreliable, axis=1)\n",
    "print(df['Unreliable'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Stanza_Score'] = df['Review'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133    Nice central location in Time Square with Pool...\n",
       "209    I was supposed to stay here the weekend of the...\n",
       "211    No service at all. Good location. No recogniti...\n",
       "265    Anthony Lydia and the front desk agents are aw...\n",
       "518    The room is too smaller for the high price, an...\n",
       "548    Clogged sink would not drain. I appreciated th...\n",
       "609    The charge for holding baggage while waiting f...\n",
       "652    just ok - not pleased with shower (very weird ...\n",
       "659    Title says it all. Better places to stay. Desk...\n",
       "947    The hotel itself seems a little dated on the i...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unreliable reviews example\n",
    "unreliable_reviews = df[df['Unreliable'] == 1]['Review']\n",
    "unreliable_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reviews rating distributions\n",
    "rating_counts = df['Review_Rating'].value_counts()\n",
    "rating_counts = rating_counts.sort_index()\n",
    "plt.figure(figsize=(8, 5))  # size\n",
    "plt.bar(rating_counts.index, rating_counts.values)\n",
    "plt.xlabel('Review_Rating')  \n",
    "plt.ylabel('Count')  \n",
    "plt.title('Distribution of Ratings')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracting(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'bout\", \"about\", text)\n",
    "    text = re.sub(r\"\\'til\", \"until\", text)\n",
    "    return text\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(tokens):\n",
    "    texts = [i for i in tokens if i not in stopwords_list]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def lemmatization(tokens):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "  return lemmatized_tokens\n",
    "\n",
    "\n",
    "def word_preprocess(df, column_name):\n",
    "  #lowercase\n",
    "  df[column_name] = df[column_name].apply(lambda x: str(x).lower())\n",
    "\n",
    "  #decontracting\n",
    "  df[column_name] = df[column_name].apply(decontracting)\n",
    "\n",
    "  #remove tags, punctuations, numbers\n",
    "  df[column_name] = df[column_name].apply(lambda x: re.sub('[^a-zA-Z!]', ' ', x))\n",
    "\n",
    "  #tokenization\n",
    "  import nltk\n",
    "  nltk.download('punkt')\n",
    "  df[column_name] = df[column_name].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "  #remove stopwords\n",
    "  df[column_name] = df[column_name].apply(remove_stopwords)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed_df = word_preprocess(df,'Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [travel, lot, general, flexible, easy, hotels,...\n",
       "1        [recently, chose, motto, overnight, nyc, glad,...\n",
       "2        [rooms, two, small, price, pay, better, hotel,...\n",
       "3        [returned, home, five, day, stay, new, york, w...\n",
       "4        [believe, overwhelming, star, reviews, propert...\n",
       "                               ...                        \n",
       "65977    [stayed, loop, suites, found, valid, !, arrive...\n",
       "65978    [enjoy, stay, !, nice, clean, good, looking, a...\n",
       "65979    [stayed, apartment, hotel, january, studio, cl...\n",
       "65980    [booked, apartment, people, couples, arrived, ...\n",
       "65981    [sorry, english, write, everyone, understand, ...\n",
       "Name: Review, Length: 65982, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nan row\n",
    "text_preprocessed_df = text_preprocessed_df.dropna(subset=['Hotel_star', 'Review_Rating', 'Review', 'Reviewer_Contributions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df['Hotel_locID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9918591822406473"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df['Review_Rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating counts:\n",
      " Review_Rating\n",
      "5    33033\n",
      "4    10399\n",
      "1     6513\n",
      "3     5773\n",
      "2     4104\n",
      "Name: count, dtype: int64\n",
      "Rating proportions:\n",
      " Review_Rating\n",
      "5    0.552188\n",
      "4    0.173832\n",
      "1    0.108873\n",
      "3    0.096503\n",
      "2    0.068604\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rating_counts = text_preprocessed_df['Review_Rating'].value_counts()\n",
    "rating_proportions = text_preprocessed_df['Review_Rating'].value_counts(normalize=True)\n",
    "\n",
    "# 打印計數和比例\n",
    "print(\"Rating counts:\\n\", rating_counts)\n",
    "print(\"Rating proportions:\\n\", rating_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets to sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_locID</th>\n",
       "      <th>Hotel_geoID_x</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Date_of_stay</th>\n",
       "      <th>Reviewer_Contributions</th>\n",
       "      <th>Reviewer_helpful_vote</th>\n",
       "      <th>Review_Rating</th>\n",
       "      <th>Trip_type</th>\n",
       "      <th>Review_helpful_votes</th>\n",
       "      <th>...</th>\n",
       "      <th>Resaurant_count</th>\n",
       "      <th>Attractions_count</th>\n",
       "      <th>Hotel_styles</th>\n",
       "      <th>Popular_mentions</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Unreliable</th>\n",
       "      <th>Stanza_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>916928494</td>\n",
       "      <td>Sep 16</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>66</td>\n",
       "      <td>Unfriendly staff and dirty</td>\n",
       "      <td>[travel, lot, general, flexible, easy, hotels,...</td>\n",
       "      <td>-0.4095</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>828890910</td>\n",
       "      <td>Feb 2022</td>\n",
       "      <td>February 2022</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>115</td>\n",
       "      <td>Perfect for Us</td>\n",
       "      <td>[recently, chose, motto, overnight, nyc, glad,...</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>915189618</td>\n",
       "      <td>Sep 6</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>99</td>\n",
       "      <td>Not satisfied with the overall experience</td>\n",
       "      <td>[rooms, two, small, price, pay, better, hotel,...</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>915010751</td>\n",
       "      <td>Sep 5</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>68</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>152</td>\n",
       "      <td>Magnificent Motto - fantastic staff</td>\n",
       "      <td>[returned, home, five, day, stay, new, york, w...</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>914794870</td>\n",
       "      <td>Sep 4</td>\n",
       "      <td>August 2023</td>\n",
       "      <td>179</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>348.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>201</td>\n",
       "      <td>Worthy of a 5-star rating!</td>\n",
       "      <td>[believe, overwhelming, star, reviews, propert...</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65954</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>17172887</td>\n",
       "      <td>Jun 2008</td>\n",
       "      <td>June 2008</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>78</td>\n",
       "      <td>Loved the Amber Inn</td>\n",
       "      <td>[loved, staying, amber, inn, !, !, !, budget, ...</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65955</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>8498082</td>\n",
       "      <td>Aug 2007</td>\n",
       "      <td>August 2007</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>48</td>\n",
       "      <td>Cannot get any better for the price</td>\n",
       "      <td>[hotel, would, recommend, anyone, staff, frien...</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65956</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>6916950</td>\n",
       "      <td>Mar 2007</td>\n",
       "      <td>March 2007</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>512</td>\n",
       "      <td>Disgusted</td>\n",
       "      <td>[family, got, room, amber, inn, room, suite, q...</td>\n",
       "      <td>-0.9605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65957</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>6898967</td>\n",
       "      <td>Feb 2007</td>\n",
       "      <td>March 2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>64</td>\n",
       "      <td>Wow! What a great place for the price!</td>\n",
       "      <td>[enjoyed, stay, amber, inn, near, southside, d...</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65958</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>4372065</td>\n",
       "      <td>Jan 2006</td>\n",
       "      <td>January 2006</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>Good space for the price</td>\n",
       "      <td>[really, enjoyed, hotel, plan, go, back, oasis...</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59822 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hotel_locID  Hotel_geoID_x  Review_id Review_Date    Date_of_stay  \\\n",
       "0         23448880          60763  916928494      Sep 16  September 2023   \n",
       "1         23448880          60763  828890910    Feb 2022   February 2022   \n",
       "2         23448880          60763  915189618       Sep 6  September 2023   \n",
       "3         23448880          60763  915010751       Sep 5  September 2023   \n",
       "4         23448880          60763  914794870       Sep 4     August 2023   \n",
       "...            ...            ...        ...         ...             ...   \n",
       "65954       290675          35805   17172887    Jun 2008       June 2008   \n",
       "65955       290675          35805    8498082    Aug 2007     August 2007   \n",
       "65956       290675          35805    6916950    Mar 2007      March 2007   \n",
       "65957       290675          35805    6898967    Feb 2007      March 2007   \n",
       "65958       290675          35805    4372065    Jan 2006    January 2006   \n",
       "\n",
       "       Reviewer_Contributions  Reviewer_helpful_vote  Review_Rating  \\\n",
       "0                           0                      0              1   \n",
       "1                          45                     26              5   \n",
       "2                           0                      0              3   \n",
       "3                          68                     88              5   \n",
       "4                         179                     68              5   \n",
       "...                       ...                    ...            ...   \n",
       "65954                       0                      2              4   \n",
       "65955                       8                      9              5   \n",
       "65956                       0                      7              1   \n",
       "65957                       0                      0              5   \n",
       "65958                       0                     33              5   \n",
       "\n",
       "                  Trip_type  Review_helpful_votes  ...  Resaurant_count  \\\n",
       "0      Traveled on business                     1  ...            348.0   \n",
       "1                       NaN                     8  ...            348.0   \n",
       "2      Traveled on business                     0  ...            348.0   \n",
       "3      Traveled with family                     1  ...            348.0   \n",
       "4             Traveled solo                     0  ...            348.0   \n",
       "...                     ...                   ...  ...              ...   \n",
       "65954  Traveled with family                     0  ...              NaN   \n",
       "65955                   NaN                     0  ...              NaN   \n",
       "65956  Traveled with family                     0  ...              NaN   \n",
       "65957  Traveled on business                     0  ...              NaN   \n",
       "65958                   NaN                     0  ...              NaN   \n",
       "\n",
       "       Attractions_count  Hotel_styles  \\\n",
       "0                  100.0  ['Business']   \n",
       "1                  100.0  ['Business']   \n",
       "2                  100.0  ['Business']   \n",
       "3                  100.0  ['Business']   \n",
       "4                  100.0  ['Business']   \n",
       "...                  ...           ...   \n",
       "65954                NaN            []   \n",
       "65955                NaN            []   \n",
       "65956                NaN            []   \n",
       "65957                NaN            []   \n",
       "65958                NaN            []   \n",
       "\n",
       "                                        Popular_mentions  WordCount  \\\n",
       "0      ['front desk', 'landmark view', 'city view roo...         66   \n",
       "1      ['front desk', 'landmark view', 'city view roo...        115   \n",
       "2      ['front desk', 'landmark view', 'city view roo...         99   \n",
       "3      ['front desk', 'landmark view', 'city view roo...        152   \n",
       "4      ['front desk', 'landmark view', 'city view roo...        201   \n",
       "...                                                  ...        ...   \n",
       "65954                                                 []         78   \n",
       "65955                                                 []         48   \n",
       "65956                                                 []        512   \n",
       "65957                                                 []         64   \n",
       "65958                                                 []        114   \n",
       "\n",
       "                                           Title  \\\n",
       "0                     Unfriendly staff and dirty   \n",
       "1                                 Perfect for Us   \n",
       "2      Not satisfied with the overall experience   \n",
       "3            Magnificent Motto - fantastic staff   \n",
       "4                     Worthy of a 5-star rating!   \n",
       "...                                          ...   \n",
       "65954                        Loved the Amber Inn   \n",
       "65955        Cannot get any better for the price   \n",
       "65956                                  Disgusted   \n",
       "65957     Wow! What a great place for the price!   \n",
       "65958                   Good space for the price   \n",
       "\n",
       "                                                  Review Compound_Score  \\\n",
       "0      [travel, lot, general, flexible, easy, hotels,...        -0.4095   \n",
       "1      [recently, chose, motto, overnight, nyc, glad,...         0.9903   \n",
       "2      [rooms, two, small, price, pay, better, hotel,...        -0.0121   \n",
       "3      [returned, home, five, day, stay, new, york, w...         0.9746   \n",
       "4      [believe, overwhelming, star, reviews, propert...         0.9949   \n",
       "...                                                  ...            ...   \n",
       "65954  [loved, staying, amber, inn, !, !, !, budget, ...         0.9112   \n",
       "65955  [hotel, would, recommend, anyone, staff, frien...         0.9485   \n",
       "65956  [family, got, room, amber, inn, room, suite, q...        -0.9605   \n",
       "65957  [enjoyed, stay, amber, inn, near, southside, d...         0.9601   \n",
       "65958  [really, enjoyed, hotel, plan, go, back, oasis...         0.9544   \n",
       "\n",
       "       Unreliable Stanza_Score  \n",
       "0               0            0  \n",
       "1               0            2  \n",
       "2               0            0  \n",
       "3               0            2  \n",
       "4               0            2  \n",
       "...           ...          ...  \n",
       "65954           0            2  \n",
       "65955           0            2  \n",
       "65956           0            0  \n",
       "65957           0            2  \n",
       "65958           0            2  \n",
       "\n",
       "[59822 rows x 24 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contributions_range(value):\n",
    "    if value <= 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def hotel_star_range(value):\n",
    "    if value <= 2.0:\n",
    "        return 1\n",
    "    elif value >= 2.5 and value <= 3.0:\n",
    "        return 1\n",
    "    elif value >= 3.5 and value <= 4.0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_738363/1093502009.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_preprocessed_df['Reviewer_Contributions_range'] = text_preprocessed_df['Reviewer_Contributions'].apply(contributions_range)\n",
      "/tmp/ipykernel_738363/1093502009.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_preprocessed_df['Hotel_star_range'] = text_preprocessed_df['Hotel_star'].apply(hotel_star_range)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_locID</th>\n",
       "      <th>Hotel_geoID_x</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Date_of_stay</th>\n",
       "      <th>Reviewer_Contributions</th>\n",
       "      <th>Reviewer_helpful_vote</th>\n",
       "      <th>Review_Rating</th>\n",
       "      <th>Trip_type</th>\n",
       "      <th>Review_helpful_votes</th>\n",
       "      <th>...</th>\n",
       "      <th>Hotel_styles</th>\n",
       "      <th>Popular_mentions</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Unreliable</th>\n",
       "      <th>Stanza_Score</th>\n",
       "      <th>Reviewer_Contributions_range</th>\n",
       "      <th>Hotel_star_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>916928494</td>\n",
       "      <td>Sep 16</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>66</td>\n",
       "      <td>Unfriendly staff and dirty</td>\n",
       "      <td>[travel, lot, general, flexible, easy, hotels,...</td>\n",
       "      <td>-0.4095</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>828890910</td>\n",
       "      <td>Feb 2022</td>\n",
       "      <td>February 2022</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>115</td>\n",
       "      <td>Perfect for Us</td>\n",
       "      <td>[recently, chose, motto, overnight, nyc, glad,...</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>915189618</td>\n",
       "      <td>Sep 6</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>99</td>\n",
       "      <td>Not satisfied with the overall experience</td>\n",
       "      <td>[rooms, two, small, price, pay, better, hotel,...</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>915010751</td>\n",
       "      <td>Sep 5</td>\n",
       "      <td>September 2023</td>\n",
       "      <td>68</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>152</td>\n",
       "      <td>Magnificent Motto - fantastic staff</td>\n",
       "      <td>[returned, home, five, day, stay, new, york, w...</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23448880</td>\n",
       "      <td>60763</td>\n",
       "      <td>914794870</td>\n",
       "      <td>Sep 4</td>\n",
       "      <td>August 2023</td>\n",
       "      <td>179</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>['Business']</td>\n",
       "      <td>['front desk', 'landmark view', 'city view roo...</td>\n",
       "      <td>201</td>\n",
       "      <td>Worthy of a 5-star rating!</td>\n",
       "      <td>[believe, overwhelming, star, reviews, propert...</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65954</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>17172887</td>\n",
       "      <td>Jun 2008</td>\n",
       "      <td>June 2008</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>78</td>\n",
       "      <td>Loved the Amber Inn</td>\n",
       "      <td>[loved, staying, amber, inn, !, !, !, budget, ...</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65955</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>8498082</td>\n",
       "      <td>Aug 2007</td>\n",
       "      <td>August 2007</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>48</td>\n",
       "      <td>Cannot get any better for the price</td>\n",
       "      <td>[hotel, would, recommend, anyone, staff, frien...</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65956</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>6916950</td>\n",
       "      <td>Mar 2007</td>\n",
       "      <td>March 2007</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>512</td>\n",
       "      <td>Disgusted</td>\n",
       "      <td>[family, got, room, amber, inn, room, suite, q...</td>\n",
       "      <td>-0.9605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65957</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>6898967</td>\n",
       "      <td>Feb 2007</td>\n",
       "      <td>March 2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>64</td>\n",
       "      <td>Wow! What a great place for the price!</td>\n",
       "      <td>[enjoyed, stay, amber, inn, near, southside, d...</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65958</th>\n",
       "      <td>290675</td>\n",
       "      <td>35805</td>\n",
       "      <td>4372065</td>\n",
       "      <td>Jan 2006</td>\n",
       "      <td>January 2006</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>Good space for the price</td>\n",
       "      <td>[really, enjoyed, hotel, plan, go, back, oasis...</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59822 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hotel_locID  Hotel_geoID_x  Review_id Review_Date    Date_of_stay  \\\n",
       "0         23448880          60763  916928494      Sep 16  September 2023   \n",
       "1         23448880          60763  828890910    Feb 2022   February 2022   \n",
       "2         23448880          60763  915189618       Sep 6  September 2023   \n",
       "3         23448880          60763  915010751       Sep 5  September 2023   \n",
       "4         23448880          60763  914794870       Sep 4     August 2023   \n",
       "...            ...            ...        ...         ...             ...   \n",
       "65954       290675          35805   17172887    Jun 2008       June 2008   \n",
       "65955       290675          35805    8498082    Aug 2007     August 2007   \n",
       "65956       290675          35805    6916950    Mar 2007      March 2007   \n",
       "65957       290675          35805    6898967    Feb 2007      March 2007   \n",
       "65958       290675          35805    4372065    Jan 2006    January 2006   \n",
       "\n",
       "       Reviewer_Contributions  Reviewer_helpful_vote  Review_Rating  \\\n",
       "0                           0                      0              1   \n",
       "1                          45                     26              5   \n",
       "2                           0                      0              3   \n",
       "3                          68                     88              5   \n",
       "4                         179                     68              5   \n",
       "...                       ...                    ...            ...   \n",
       "65954                       0                      2              4   \n",
       "65955                       8                      9              5   \n",
       "65956                       0                      7              1   \n",
       "65957                       0                      0              5   \n",
       "65958                       0                     33              5   \n",
       "\n",
       "                  Trip_type  Review_helpful_votes  ...  Hotel_styles  \\\n",
       "0      Traveled on business                     1  ...  ['Business']   \n",
       "1                       NaN                     8  ...  ['Business']   \n",
       "2      Traveled on business                     0  ...  ['Business']   \n",
       "3      Traveled with family                     1  ...  ['Business']   \n",
       "4             Traveled solo                     0  ...  ['Business']   \n",
       "...                     ...                   ...  ...           ...   \n",
       "65954  Traveled with family                     0  ...            []   \n",
       "65955                   NaN                     0  ...            []   \n",
       "65956  Traveled with family                     0  ...            []   \n",
       "65957  Traveled on business                     0  ...            []   \n",
       "65958                   NaN                     0  ...            []   \n",
       "\n",
       "                                        Popular_mentions  WordCount  \\\n",
       "0      ['front desk', 'landmark view', 'city view roo...         66   \n",
       "1      ['front desk', 'landmark view', 'city view roo...        115   \n",
       "2      ['front desk', 'landmark view', 'city view roo...         99   \n",
       "3      ['front desk', 'landmark view', 'city view roo...        152   \n",
       "4      ['front desk', 'landmark view', 'city view roo...        201   \n",
       "...                                                  ...        ...   \n",
       "65954                                                 []         78   \n",
       "65955                                                 []         48   \n",
       "65956                                                 []        512   \n",
       "65957                                                 []         64   \n",
       "65958                                                 []        114   \n",
       "\n",
       "                                           Title  \\\n",
       "0                     Unfriendly staff and dirty   \n",
       "1                                 Perfect for Us   \n",
       "2      Not satisfied with the overall experience   \n",
       "3            Magnificent Motto - fantastic staff   \n",
       "4                     Worthy of a 5-star rating!   \n",
       "...                                          ...   \n",
       "65954                        Loved the Amber Inn   \n",
       "65955        Cannot get any better for the price   \n",
       "65956                                  Disgusted   \n",
       "65957     Wow! What a great place for the price!   \n",
       "65958                   Good space for the price   \n",
       "\n",
       "                                                  Review  Compound_Score  \\\n",
       "0      [travel, lot, general, flexible, easy, hotels,...         -0.4095   \n",
       "1      [recently, chose, motto, overnight, nyc, glad,...          0.9903   \n",
       "2      [rooms, two, small, price, pay, better, hotel,...         -0.0121   \n",
       "3      [returned, home, five, day, stay, new, york, w...          0.9746   \n",
       "4      [believe, overwhelming, star, reviews, propert...          0.9949   \n",
       "...                                                  ...             ...   \n",
       "65954  [loved, staying, amber, inn, !, !, !, budget, ...          0.9112   \n",
       "65955  [hotel, would, recommend, anyone, staff, frien...          0.9485   \n",
       "65956  [family, got, room, amber, inn, room, suite, q...         -0.9605   \n",
       "65957  [enjoyed, stay, amber, inn, near, southside, d...          0.9601   \n",
       "65958  [really, enjoyed, hotel, plan, go, back, oasis...          0.9544   \n",
       "\n",
       "      Unreliable Stanza_Score  Reviewer_Contributions_range Hotel_star_range  \n",
       "0              0            0                             1                2  \n",
       "1              0            2                             2                2  \n",
       "2              0            0                             1                2  \n",
       "3              0            2                             2                2  \n",
       "4              0            2                             2                2  \n",
       "...          ...          ...                           ...              ...  \n",
       "65954          0            2                             1                1  \n",
       "65955          0            2                             2                1  \n",
       "65956          0            0                             1                1  \n",
       "65957          0            2                             1                1  \n",
       "65958          0            2                             1                1  \n",
       "\n",
       "[59822 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new columns\n",
    "text_preprocessed_df['Reviewer_Contributions_range'] = text_preprocessed_df['Reviewer_Contributions'].apply(contributions_range)\n",
    "text_preprocessed_df['Hotel_star_range'] = text_preprocessed_df['Hotel_star'].apply(hotel_star_range)\n",
    "text_preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 4 sub-datasets\n",
    "LCLS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 1) & (text_preprocessed_df['Hotel_star_range'] == 1)]\n",
    "LCHS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 1) & (text_preprocessed_df['Hotel_star_range'] == 2)]\n",
    "HCLS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 2) & (text_preprocessed_df['Hotel_star_range'] == 1)]\n",
    "HCHS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 2) & (text_preprocessed_df['Hotel_star_range'] == 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split unreliable reviews\n",
    "LCLS_unreliable = LCLS[LCLS['Unreliable'] == 1]\n",
    "LCHS_unreliable = LCHS[LCHS['Unreliable'] == 1]\n",
    "HCLS_unreliable = HCLS[HCLS['Unreliable'] == 1]\n",
    "HCHS_unreliable = HCHS[HCHS['Unreliable'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split reliable reviews\n",
    "LCLS_reliable = LCLS[LCLS['Unreliable'] == 0]\n",
    "LCHS_reliable = LCHS[LCHS['Unreliable'] == 0]\n",
    "HCLS_reliable = HCLS[HCLS['Unreliable'] == 0]\n",
    "HCHS_reliable = HCHS[HCHS['Unreliable'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split subset\n",
    "selected_columns = ['Review', 'Review_Rating']\n",
    "LCLS_text = LCLS_reliable.loc[:, selected_columns]\n",
    "LCHS_text = LCHS_reliable.loc[:, selected_columns]\n",
    "HCLS_text = HCLS_reliable.loc[:, selected_columns]\n",
    "HCHS_text = HCHS_reliable.loc[:, selected_columns]\n",
    "\n",
    "# reset index\n",
    "LCLS_text.reset_index(drop=True, inplace=True)\n",
    "LCHS_text.reset_index(drop=True, inplace=True)\n",
    "HCLS_text.reset_index(drop=True, inplace=True)\n",
    "HCHS_text.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_text['Review'] = [' '.join(text) for text in LCLS_text['Review']]\n",
    "LCHS_text['Review'] = [' '.join(text) for text in LCHS_text['Review']]\n",
    "HCLS_text['Review'] = [' '.join(text) for text in HCLS_text['Review']]\n",
    "HCHS_text['Review'] = [' '.join(text) for text in HCHS_text['Review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split sub-datasets to X & Y, Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X = LCLS_text['Review']\n",
    "LCLS_y = LCLS_text['Review_Rating']\n",
    "\n",
    "LCHS_X = LCHS_text['Review']\n",
    "LCHS_y = LCHS_text['Review_Rating']\n",
    "\n",
    "HCLS_X = HCLS_text['Review']\n",
    "HCLS_y = HCLS_text['Review_Rating']\n",
    "\n",
    "HCHS_X = HCHS_text['Review']\n",
    "HCHS_y = HCHS_text['Review_Rating']\n",
    "\n",
    "LCLS_X_train, LCLS_X_test, LCLS_y_train, LCLS_y_test = train_test_split(LCLS_X, LCLS_y, test_size=0.2, random_state=88)\n",
    "LCHS_X_train, LCHS_X_test, LCHS_y_train, LCHS_y_test = train_test_split(LCHS_X, LCHS_y, test_size=0.2, random_state=88)\n",
    "HCLS_X_train, HCLS_X_test, HCLS_y_train, HCLS_y_test = train_test_split(HCLS_X, HCLS_y, test_size=0.2, random_state=88)\n",
    "HCHS_X_train, HCHS_X_test, HCHS_y_train, HCHS_y_test = train_test_split(HCHS_X, HCHS_y, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10068    know great stay suite constant cannabis odor c...\n",
       "8028     stayed five nights chose proximity broadway wa...\n",
       "12388    enjoyed stay valet parking could better check ...\n",
       "8983     experience hotel never book another hotel onli...\n",
       "10036    love hotel ! place always clean staff awesome ...\n",
       "                               ...                        \n",
       "8554     room clean cleaned daily basis despite room le...\n",
       "2481     checked hotel around pm greeted friendly staff...\n",
       "4047     basic hotel underwhelming hilton property lobb...\n",
       "6432     first trip hotel since start pandemic stayed h...\n",
       "10200    bad experience hotel stayed hotel one night bo...\n",
       "Name: Review, Length: 11072, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCLS_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10068    2\n",
       "8028     3\n",
       "12388    4\n",
       "8983     1\n",
       "10036    5\n",
       "        ..\n",
       "8554     5\n",
       "2481     4\n",
       "4047     3\n",
       "6432     1\n",
       "10200    1\n",
       "Name: Review_Rating, Length: 11072, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCLS_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS \n",
    "LCLS_X_train_bow_df = None\n",
    "LCLS_X_train_tfidf_df = None\n",
    "LCLS_X_train_d2v = None\n",
    "LCLS_X_train_glove = None\n",
    "LCLS_X_train_bert = None\n",
    "LCLS_X_test_bow_df = None\n",
    "LCLS_X_test_tfidf_df = None\n",
    "LCLS_X_test_d2v = None\n",
    "LCLS_X_test_glove = None\n",
    "LCLS_X_test_bert = None\n",
    "\n",
    "# LCHS\n",
    "LCHS_X_train_bow_df = None\n",
    "LCHS_X_train_tfidf_df = None\n",
    "LCHS_X_train_d2v = None\n",
    "LCHS_X_train_glove = None\n",
    "LCHS_X_train_bert = None\n",
    "LCHS_X_test_bow_df = None\n",
    "LCHS_X_test_tfidf_df = None\n",
    "LCHS_X_test_d2v = None\n",
    "LCHS_X_test_glove = None\n",
    "LCHS_X_test_bert = None\n",
    "\n",
    "# HCLS\n",
    "HCLS_X_train_bow_df = None\n",
    "HCLS_X_train_tfidf_df = None\n",
    "HCLS_X_train_d2v = None\n",
    "HCLS_X_train_glove = None\n",
    "HCLS_X_train_bert = None\n",
    "HCLS_X_test_bow_df = None\n",
    "HCLS_X_test_tfidf_df = None\n",
    "HCLS_X_test_d2v = None\n",
    "HCLS_X_test_glove = None\n",
    "HCLS_X_test_bert = None\n",
    "\n",
    "# HCHS\n",
    "HCHS_X_train_bow_df = None\n",
    "HCHS_X_train_tfidf_df = None\n",
    "HCHS_X_train_d2v = None\n",
    "HCHS_X_train_glove = None\n",
    "HCHS_X_train_bert = None\n",
    "HCHS_X_test_bow_df = None\n",
    "HCHS_X_test_tfidf_df = None\n",
    "HCHS_X_test_d2v = None\n",
    "HCHS_X_test_glove = None\n",
    "HCHS_X_test_bert = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def generate_bow_train(X_train, max_features=1000):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    bow_vectors = vectorizer.fit_transform(X_train)\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bow_df, vectorizer\n",
    "\n",
    "def generate_bow_test(X_test, vectorizer):\n",
    "    bow_vectors = vectorizer.transform(X_test)\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_bow_df, LCLS_vectorizer = generate_bow_train(LCLS_X_train)\n",
    "LCHS_X_train_bow_df, LCHS_vectorizer = generate_bow_train(LCHS_X_train)\n",
    "HCLS_X_train_bow_df, HCLS_vectorizer = generate_bow_train(HCLS_X_train)\n",
    "HCHS_X_train_bow_df, HCHS_vectorizer = generate_bow_train(HCHS_X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_bow_df = generate_bow_test(LCLS_X_test, LCLS_vectorizer)\n",
    "LCHS_X_test_bow_df = generate_bow_test(LCHS_X_test, LCHS_vectorizer)\n",
    "HCLS_X_test_bow_df = generate_bow_test(HCLS_X_test, HCLS_vectorizer)\n",
    "HCHS_X_test_bow_df = generate_bow_test(HCHS_X_test, HCHS_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def generate_tfidf_train(X_train, stop_words='english', max_features=1000, max_df=0.9):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features, max_df=max_df)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(X_train_tfidf, columns=feature_names)\n",
    "    \n",
    "    return tfidf_df, tfidf_vectorizer\n",
    "\n",
    "def generate_tfidf_test(X_test, tfidf_vectorizer):\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(X_test_tfidf, columns=feature_names)\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_tfidf_df, tfidf_vectorizer_LCLS = generate_tfidf_train(LCLS_X_train)\n",
    "LCHS_X_train_tfidf_df, tfidf_vectorizer_LCHS = generate_tfidf_train(LCHS_X_train)\n",
    "HCLS_X_train_tfidf_df, tfidf_vectorizer_HCLS = generate_tfidf_train(HCLS_X_train)\n",
    "HCHS_X_train_tfidf_df, tfidf_vectorizer_HCHS = generate_tfidf_train(HCHS_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_tfidf_df = generate_tfidf_test(LCLS_X_test, tfidf_vectorizer_LCLS)\n",
    "LCHS_X_test_tfidf_df = generate_tfidf_test(LCHS_X_test, tfidf_vectorizer_LCHS)\n",
    "HCLS_X_test_tfidf_df = generate_tfidf_test(HCLS_X_test, tfidf_vectorizer_HCLS)\n",
    "HCHS_X_test_tfidf_df = generate_tfidf_test(HCHS_X_test, tfidf_vectorizer_HCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def train_doc2vec_model(X_train, min_count=5, workers=8, epochs=40, vector_size=100):\n",
    "    tagged_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(X_train)]\n",
    "    model = Doc2Vec(min_count=min_count, workers=workers, epochs=epochs, vector_size=vector_size)\n",
    "    model.build_vocab(tagged_docs)\n",
    "    model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different datasets of Doc2vec\n",
    "LCLS_model = train_doc2vec_model(LCLS_X_train)\n",
    "LCHS_model = train_doc2vec_model(LCHS_X_train)\n",
    "HCLS_model = train_doc2vec_model(HCLS_X_train)\n",
    "HCHS_model = train_doc2vec_model(HCHS_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_d2v = pd.DataFrame(np.array([LCLS_model.infer_vector((doc.split(' '))) for doc in LCLS_X_train]))\n",
    "LCHS_X_train_d2v = pd.DataFrame(np.array([LCHS_model.infer_vector((doc.split(' '))) for doc in LCHS_X_train]))\n",
    "HCLS_X_train_d2v = pd.DataFrame(np.array([HCLS_model.infer_vector((doc.split(' '))) for doc in HCLS_X_train]))\n",
    "HCHS_X_train_d2v = pd.DataFrame(np.array([HCHS_model.infer_vector((doc.split(' '))) for doc in HCHS_X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.523082</td>\n",
       "      <td>0.296648</td>\n",
       "      <td>-1.133473</td>\n",
       "      <td>-0.387598</td>\n",
       "      <td>0.692584</td>\n",
       "      <td>0.114354</td>\n",
       "      <td>-1.224200</td>\n",
       "      <td>0.031827</td>\n",
       "      <td>-0.597348</td>\n",
       "      <td>0.352433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500912</td>\n",
       "      <td>0.300213</td>\n",
       "      <td>0.238977</td>\n",
       "      <td>-0.029682</td>\n",
       "      <td>0.828570</td>\n",
       "      <td>1.391957</td>\n",
       "      <td>-0.281398</td>\n",
       "      <td>0.277845</td>\n",
       "      <td>0.352651</td>\n",
       "      <td>-0.035584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.259783</td>\n",
       "      <td>0.904154</td>\n",
       "      <td>0.793139</td>\n",
       "      <td>0.662008</td>\n",
       "      <td>-0.117537</td>\n",
       "      <td>-0.666306</td>\n",
       "      <td>-0.453616</td>\n",
       "      <td>0.369642</td>\n",
       "      <td>-0.502035</td>\n",
       "      <td>-0.240692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.387303</td>\n",
       "      <td>1.120812</td>\n",
       "      <td>-0.106885</td>\n",
       "      <td>0.596611</td>\n",
       "      <td>0.409919</td>\n",
       "      <td>1.067631</td>\n",
       "      <td>0.605203</td>\n",
       "      <td>-0.542012</td>\n",
       "      <td>0.246398</td>\n",
       "      <td>-0.270192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.455936</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.991037</td>\n",
       "      <td>0.694886</td>\n",
       "      <td>-0.518279</td>\n",
       "      <td>0.103095</td>\n",
       "      <td>0.437357</td>\n",
       "      <td>-0.148169</td>\n",
       "      <td>0.634140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055327</td>\n",
       "      <td>1.466131</td>\n",
       "      <td>1.625989</td>\n",
       "      <td>-0.333979</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.755785</td>\n",
       "      <td>-0.613750</td>\n",
       "      <td>-0.480121</td>\n",
       "      <td>0.357386</td>\n",
       "      <td>0.172741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.178525</td>\n",
       "      <td>-0.753358</td>\n",
       "      <td>0.049491</td>\n",
       "      <td>0.036924</td>\n",
       "      <td>-0.377599</td>\n",
       "      <td>-0.375430</td>\n",
       "      <td>0.097864</td>\n",
       "      <td>0.766669</td>\n",
       "      <td>-0.470298</td>\n",
       "      <td>0.277646</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087451</td>\n",
       "      <td>0.420021</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>-0.241431</td>\n",
       "      <td>1.037216</td>\n",
       "      <td>0.106721</td>\n",
       "      <td>-0.160821</td>\n",
       "      <td>0.114221</td>\n",
       "      <td>0.473653</td>\n",
       "      <td>0.168597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.272393</td>\n",
       "      <td>-0.711800</td>\n",
       "      <td>-0.227729</td>\n",
       "      <td>-0.143623</td>\n",
       "      <td>0.753441</td>\n",
       "      <td>-0.865689</td>\n",
       "      <td>0.609237</td>\n",
       "      <td>0.265007</td>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.412447</td>\n",
       "      <td>...</td>\n",
       "      <td>1.026637</td>\n",
       "      <td>1.769429</td>\n",
       "      <td>-0.688688</td>\n",
       "      <td>1.056201</td>\n",
       "      <td>1.057315</td>\n",
       "      <td>0.218610</td>\n",
       "      <td>0.346593</td>\n",
       "      <td>-0.258446</td>\n",
       "      <td>-0.071336</td>\n",
       "      <td>-1.018418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10165</th>\n",
       "      <td>0.387317</td>\n",
       "      <td>0.732917</td>\n",
       "      <td>-0.403494</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>-1.068793</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>-0.254361</td>\n",
       "      <td>0.556155</td>\n",
       "      <td>-0.225291</td>\n",
       "      <td>-1.269369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301838</td>\n",
       "      <td>0.220722</td>\n",
       "      <td>0.242965</td>\n",
       "      <td>-0.463811</td>\n",
       "      <td>0.976006</td>\n",
       "      <td>-0.125246</td>\n",
       "      <td>0.081194</td>\n",
       "      <td>-0.126672</td>\n",
       "      <td>0.157977</td>\n",
       "      <td>1.255168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10166</th>\n",
       "      <td>0.394923</td>\n",
       "      <td>0.385681</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>-0.135913</td>\n",
       "      <td>0.626152</td>\n",
       "      <td>0.356390</td>\n",
       "      <td>-0.067197</td>\n",
       "      <td>-0.076829</td>\n",
       "      <td>-0.604196</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527894</td>\n",
       "      <td>0.620011</td>\n",
       "      <td>0.556896</td>\n",
       "      <td>0.319913</td>\n",
       "      <td>1.270682</td>\n",
       "      <td>0.042770</td>\n",
       "      <td>0.222717</td>\n",
       "      <td>-0.171083</td>\n",
       "      <td>0.033395</td>\n",
       "      <td>-0.044318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10167</th>\n",
       "      <td>-0.984939</td>\n",
       "      <td>-0.117823</td>\n",
       "      <td>0.300837</td>\n",
       "      <td>-0.680077</td>\n",
       "      <td>-0.653513</td>\n",
       "      <td>-0.262709</td>\n",
       "      <td>0.145111</td>\n",
       "      <td>0.486231</td>\n",
       "      <td>-1.926945</td>\n",
       "      <td>0.884870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083981</td>\n",
       "      <td>0.297135</td>\n",
       "      <td>0.449036</td>\n",
       "      <td>0.304024</td>\n",
       "      <td>1.091182</td>\n",
       "      <td>-0.447719</td>\n",
       "      <td>0.574557</td>\n",
       "      <td>0.384139</td>\n",
       "      <td>0.655918</td>\n",
       "      <td>-0.626324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10168</th>\n",
       "      <td>-0.457017</td>\n",
       "      <td>0.196348</td>\n",
       "      <td>0.352738</td>\n",
       "      <td>0.803386</td>\n",
       "      <td>0.200494</td>\n",
       "      <td>-2.169582</td>\n",
       "      <td>0.422091</td>\n",
       "      <td>0.032375</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365549</td>\n",
       "      <td>1.817679</td>\n",
       "      <td>1.184606</td>\n",
       "      <td>0.307011</td>\n",
       "      <td>-0.151900</td>\n",
       "      <td>-0.092502</td>\n",
       "      <td>0.176244</td>\n",
       "      <td>-0.395211</td>\n",
       "      <td>0.699995</td>\n",
       "      <td>-0.724225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10169</th>\n",
       "      <td>0.012254</td>\n",
       "      <td>-0.584637</td>\n",
       "      <td>-0.748910</td>\n",
       "      <td>0.301891</td>\n",
       "      <td>0.623112</td>\n",
       "      <td>-0.922393</td>\n",
       "      <td>0.246986</td>\n",
       "      <td>0.563931</td>\n",
       "      <td>-0.503263</td>\n",
       "      <td>-0.559869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198601</td>\n",
       "      <td>0.591236</td>\n",
       "      <td>-0.524530</td>\n",
       "      <td>-0.632589</td>\n",
       "      <td>0.807704</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>-0.083875</td>\n",
       "      <td>1.360660</td>\n",
       "      <td>0.235374</td>\n",
       "      <td>-0.327722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10170 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.523082  0.296648 -1.133473 -0.387598  0.692584  0.114354 -1.224200   \n",
       "1     -0.259783  0.904154  0.793139  0.662008 -0.117537 -0.666306 -0.453616   \n",
       "2      0.787728  0.455936  0.022394  0.991037  0.694886 -0.518279  0.103095   \n",
       "3      0.178525 -0.753358  0.049491  0.036924 -0.377599 -0.375430  0.097864   \n",
       "4     -0.272393 -0.711800 -0.227729 -0.143623  0.753441 -0.865689  0.609237   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10165  0.387317  0.732917 -0.403494  0.016972 -1.068793  0.025481 -0.254361   \n",
       "10166  0.394923  0.385681  0.014842 -0.135913  0.626152  0.356390 -0.067197   \n",
       "10167 -0.984939 -0.117823  0.300837 -0.680077 -0.653513 -0.262709  0.145111   \n",
       "10168 -0.457017  0.196348  0.352738  0.803386  0.200494 -2.169582  0.422091   \n",
       "10169  0.012254 -0.584637 -0.748910  0.301891  0.623112 -0.922393  0.246986   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.031827 -0.597348  0.352433  ...  0.500912  0.300213  0.238977   \n",
       "1      0.369642 -0.502035 -0.240692  ... -0.387303  1.120812 -0.106885   \n",
       "2      0.437357 -0.148169  0.634140  ...  0.055327  1.466131  1.625989   \n",
       "3      0.766669 -0.470298  0.277646  ... -0.087451  0.420021 -0.029921   \n",
       "4      0.265007  0.032349  0.412447  ...  1.026637  1.769429 -0.688688   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10165  0.556155 -0.225291 -1.269369  ... -0.301838  0.220722  0.242965   \n",
       "10166 -0.076829 -0.604196  0.013508  ...  0.527894  0.620011  0.556896   \n",
       "10167  0.486231 -1.926945  0.884870  ... -0.083981  0.297135  0.449036   \n",
       "10168  0.032375  0.745375  0.212900  ...  0.365549  1.817679  1.184606   \n",
       "10169  0.563931 -0.503263 -0.559869  ... -0.198601  0.591236 -0.524530   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0     -0.029682  0.828570  1.391957 -0.281398  0.277845  0.352651 -0.035584  \n",
       "1      0.596611  0.409919  1.067631  0.605203 -0.542012  0.246398 -0.270192  \n",
       "2     -0.333979  0.278431  0.755785 -0.613750 -0.480121  0.357386  0.172741  \n",
       "3     -0.241431  1.037216  0.106721 -0.160821  0.114221  0.473653  0.168597  \n",
       "4      1.056201  1.057315  0.218610  0.346593 -0.258446 -0.071336 -1.018418  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10165 -0.463811  0.976006 -0.125246  0.081194 -0.126672  0.157977  1.255168  \n",
       "10166  0.319913  1.270682  0.042770  0.222717 -0.171083  0.033395 -0.044318  \n",
       "10167  0.304024  1.091182 -0.447719  0.574557  0.384139  0.655918 -0.626324  \n",
       "10168  0.307011 -0.151900 -0.092502  0.176244 -0.395211  0.699995 -0.724225  \n",
       "10169 -0.632589  0.807704  0.366516 -0.083875  1.360660  0.235374 -0.327722  \n",
       "\n",
       "[10170 rows x 100 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCHS_X_train_d2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_d2v = np.array([LCLS_model.infer_vector((doc.split(' '))) for doc in LCLS_X_test])\n",
    "LCHS_X_test_d2v = np.array([LCHS_model.infer_vector((doc.split(' '))) for doc in LCHS_X_test])\n",
    "HCLS_X_test_d2v = np.array([HCLS_model.infer_vector((doc.split(' '))) for doc in HCLS_X_test])\n",
    "HCHS_X_test_d2v = np.array([HCHS_model.infer_vector((doc.split(' '))) for doc in HCHS_X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../GloVe_wordvec/glove.6B.100d.txt'\n",
    "\n",
    "# import GloVe word vectors into dictionary\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create embedding matrix\n",
    "def glove_embedding(comment, embeddings_index = embeddings_index, dim=100):\n",
    "    words = comment.split()\n",
    "    vec = np.zeros(dim)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word) # get GloVe word vectors\n",
    "        if embedding_vector is not None:\n",
    "            vec += embedding_vector\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "LCLS_X_train_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in LCLS_X_train]))\n",
    "LCHS_X_train_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in LCHS_X_train]))\n",
    "HCLS_X_train_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in HCLS_X_train]))\n",
    "HCHS_X_train_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in HCHS_X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023992</td>\n",
       "      <td>-0.039079</td>\n",
       "      <td>0.296264</td>\n",
       "      <td>-0.237071</td>\n",
       "      <td>-0.151624</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>-0.066503</td>\n",
       "      <td>0.309266</td>\n",
       "      <td>0.085064</td>\n",
       "      <td>-0.064599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236503</td>\n",
       "      <td>0.015690</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>-0.060600</td>\n",
       "      <td>-0.255384</td>\n",
       "      <td>0.082591</td>\n",
       "      <td>-0.006289</td>\n",
       "      <td>-0.009381</td>\n",
       "      <td>0.337734</td>\n",
       "      <td>0.138108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.170518</td>\n",
       "      <td>0.074603</td>\n",
       "      <td>0.238125</td>\n",
       "      <td>-0.102362</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.168701</td>\n",
       "      <td>-0.132858</td>\n",
       "      <td>0.432343</td>\n",
       "      <td>-0.010722</td>\n",
       "      <td>0.022450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011282</td>\n",
       "      <td>-0.108942</td>\n",
       "      <td>0.071287</td>\n",
       "      <td>-0.174182</td>\n",
       "      <td>-0.568867</td>\n",
       "      <td>0.140666</td>\n",
       "      <td>-0.113348</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.192074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.093806</td>\n",
       "      <td>-0.060850</td>\n",
       "      <td>0.296436</td>\n",
       "      <td>-0.179451</td>\n",
       "      <td>-0.076238</td>\n",
       "      <td>0.169154</td>\n",
       "      <td>-0.136328</td>\n",
       "      <td>0.469972</td>\n",
       "      <td>0.290695</td>\n",
       "      <td>-0.014746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162560</td>\n",
       "      <td>-0.113817</td>\n",
       "      <td>0.327392</td>\n",
       "      <td>-0.156790</td>\n",
       "      <td>-0.364735</td>\n",
       "      <td>0.076559</td>\n",
       "      <td>-0.022164</td>\n",
       "      <td>0.058268</td>\n",
       "      <td>0.552769</td>\n",
       "      <td>0.123194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.065803</td>\n",
       "      <td>0.181605</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>-0.207001</td>\n",
       "      <td>-0.128023</td>\n",
       "      <td>0.243699</td>\n",
       "      <td>-0.021704</td>\n",
       "      <td>0.350799</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>-0.052574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101987</td>\n",
       "      <td>-0.142861</td>\n",
       "      <td>0.171897</td>\n",
       "      <td>-0.124271</td>\n",
       "      <td>-0.355647</td>\n",
       "      <td>-0.017630</td>\n",
       "      <td>-0.067118</td>\n",
       "      <td>-0.064801</td>\n",
       "      <td>0.424609</td>\n",
       "      <td>0.083366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.017846</td>\n",
       "      <td>0.218327</td>\n",
       "      <td>0.350826</td>\n",
       "      <td>-0.289497</td>\n",
       "      <td>-0.234915</td>\n",
       "      <td>0.188229</td>\n",
       "      <td>-0.186909</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>-0.047220</td>\n",
       "      <td>-0.128190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052861</td>\n",
       "      <td>-0.152064</td>\n",
       "      <td>0.287734</td>\n",
       "      <td>0.098489</td>\n",
       "      <td>-0.552768</td>\n",
       "      <td>-0.017102</td>\n",
       "      <td>-0.160565</td>\n",
       "      <td>-0.188943</td>\n",
       "      <td>0.389988</td>\n",
       "      <td>0.338062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11067</th>\n",
       "      <td>-0.060565</td>\n",
       "      <td>0.237114</td>\n",
       "      <td>0.272883</td>\n",
       "      <td>-0.207138</td>\n",
       "      <td>-0.125995</td>\n",
       "      <td>0.170181</td>\n",
       "      <td>-0.074178</td>\n",
       "      <td>0.165311</td>\n",
       "      <td>-0.009205</td>\n",
       "      <td>-0.076757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048294</td>\n",
       "      <td>-0.074803</td>\n",
       "      <td>0.110280</td>\n",
       "      <td>-0.183655</td>\n",
       "      <td>-0.484453</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>-0.042739</td>\n",
       "      <td>-0.114552</td>\n",
       "      <td>0.410044</td>\n",
       "      <td>0.215139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11068</th>\n",
       "      <td>-0.124657</td>\n",
       "      <td>0.101271</td>\n",
       "      <td>0.219382</td>\n",
       "      <td>-0.042047</td>\n",
       "      <td>-0.213371</td>\n",
       "      <td>0.072996</td>\n",
       "      <td>0.065513</td>\n",
       "      <td>0.383784</td>\n",
       "      <td>-0.161596</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281881</td>\n",
       "      <td>-0.091722</td>\n",
       "      <td>0.114590</td>\n",
       "      <td>-0.027783</td>\n",
       "      <td>-0.483824</td>\n",
       "      <td>0.136178</td>\n",
       "      <td>-0.088659</td>\n",
       "      <td>-0.035635</td>\n",
       "      <td>0.560823</td>\n",
       "      <td>0.141612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11069</th>\n",
       "      <td>-0.068153</td>\n",
       "      <td>0.123244</td>\n",
       "      <td>0.185302</td>\n",
       "      <td>-0.127660</td>\n",
       "      <td>-0.124014</td>\n",
       "      <td>0.292227</td>\n",
       "      <td>0.032201</td>\n",
       "      <td>0.365379</td>\n",
       "      <td>0.024816</td>\n",
       "      <td>-0.012145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106635</td>\n",
       "      <td>-0.116547</td>\n",
       "      <td>0.189137</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>-0.432882</td>\n",
       "      <td>0.065838</td>\n",
       "      <td>-0.122937</td>\n",
       "      <td>-0.089474</td>\n",
       "      <td>0.472984</td>\n",
       "      <td>0.190128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11070</th>\n",
       "      <td>-0.050511</td>\n",
       "      <td>0.098430</td>\n",
       "      <td>0.239151</td>\n",
       "      <td>-0.125203</td>\n",
       "      <td>-0.084869</td>\n",
       "      <td>0.214339</td>\n",
       "      <td>0.015258</td>\n",
       "      <td>0.309809</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187991</td>\n",
       "      <td>-0.002887</td>\n",
       "      <td>0.084901</td>\n",
       "      <td>-0.023960</td>\n",
       "      <td>-0.421153</td>\n",
       "      <td>0.020129</td>\n",
       "      <td>-0.021075</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.326544</td>\n",
       "      <td>0.079239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11071</th>\n",
       "      <td>0.112335</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.338696</td>\n",
       "      <td>-0.163885</td>\n",
       "      <td>-0.203661</td>\n",
       "      <td>0.371996</td>\n",
       "      <td>-0.105371</td>\n",
       "      <td>0.332208</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>-0.050124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051461</td>\n",
       "      <td>-0.253291</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>-0.022236</td>\n",
       "      <td>-0.306677</td>\n",
       "      <td>0.076515</td>\n",
       "      <td>-0.224527</td>\n",
       "      <td>-0.057302</td>\n",
       "      <td>0.373827</td>\n",
       "      <td>0.138550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11072 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.023992 -0.039079  0.296264 -0.237071 -0.151624  0.340974 -0.066503   \n",
       "1     -0.170518  0.074603  0.238125 -0.102362  0.001733  0.168701 -0.132858   \n",
       "2     -0.093806 -0.060850  0.296436 -0.179451 -0.076238  0.169154 -0.136328   \n",
       "3     -0.065803  0.181605  0.270492 -0.207001 -0.128023  0.243699 -0.021704   \n",
       "4     -0.017846  0.218327  0.350826 -0.289497 -0.234915  0.188229 -0.186909   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11067 -0.060565  0.237114  0.272883 -0.207138 -0.125995  0.170181 -0.074178   \n",
       "11068 -0.124657  0.101271  0.219382 -0.042047 -0.213371  0.072996  0.065513   \n",
       "11069 -0.068153  0.123244  0.185302 -0.127660 -0.124014  0.292227  0.032201   \n",
       "11070 -0.050511  0.098430  0.239151 -0.125203 -0.084869  0.214339  0.015258   \n",
       "11071  0.112335  0.004213  0.338696 -0.163885 -0.203661  0.371996 -0.105371   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.309266  0.085064 -0.064599  ...  0.236503  0.015690  0.140292   \n",
       "1      0.432343 -0.010722  0.022450  ...  0.011282 -0.108942  0.071287   \n",
       "2      0.469972  0.290695 -0.014746  ...  0.162560 -0.113817  0.327392   \n",
       "3      0.350799  0.040657 -0.052574  ...  0.101987 -0.142861  0.171897   \n",
       "4      0.252001 -0.047220 -0.128190  ...  0.052861 -0.152064  0.287734   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "11067  0.165311 -0.009205 -0.076757  ...  0.048294 -0.074803  0.110280   \n",
       "11068  0.383784 -0.161596  0.026596  ...  0.281881 -0.091722  0.114590   \n",
       "11069  0.365379  0.024816 -0.012145  ...  0.106635 -0.116547  0.189137   \n",
       "11070  0.309809  0.020759  0.001958  ...  0.187991 -0.002887  0.084901   \n",
       "11071  0.332208  0.023441 -0.050124  ...  0.051461 -0.253291  0.150022   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0     -0.060600 -0.255384  0.082591 -0.006289 -0.009381  0.337734  0.138108  \n",
       "1     -0.174182 -0.568867  0.140666 -0.113348  0.002950  0.374100  0.192074  \n",
       "2     -0.156790 -0.364735  0.076559 -0.022164  0.058268  0.552769  0.123194  \n",
       "3     -0.124271 -0.355647 -0.017630 -0.067118 -0.064801  0.424609  0.083366  \n",
       "4      0.098489 -0.552768 -0.017102 -0.160565 -0.188943  0.389988  0.338062  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "11067 -0.183655 -0.484453  0.004562 -0.042739 -0.114552  0.410044  0.215139  \n",
       "11068 -0.027783 -0.483824  0.136178 -0.088659 -0.035635  0.560823  0.141612  \n",
       "11069  0.008954 -0.432882  0.065838 -0.122937 -0.089474  0.472984  0.190128  \n",
       "11070 -0.023960 -0.421153  0.020129 -0.021075  0.013633  0.326544  0.079239  \n",
       "11071 -0.022236 -0.306677  0.076515 -0.224527 -0.057302  0.373827  0.138550  \n",
       "\n",
       "[11072 rows x 100 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCLS_X_train_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "LCLS_X_test_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in LCLS_X_test]))\n",
    "LCHS_X_test_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in LCHS_X_test]))\n",
    "HCLS_X_test_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in HCLS_X_test]))\n",
    "HCHS_X_test_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in HCHS_X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT(longformer) model\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = transformers.LongformerTokenizer.from_pretrained(model_name)\n",
    "model = transformers.LongformerModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def bert_embedding(X_train):\n",
    "    embeddings = []\n",
    "    for text in X_train:\n",
    "        # 將文本轉成BERT的輸入格式，即加上[CLS]與[SEP] token，並轉成tensor\n",
    "        encoded_text = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # 用預訓練BERT模型轉成向量\n",
    "        with torch.no_grad():\n",
    "            model_output = model(encoded_text['input_ids'], attention_mask=encoded_text['attention_mask'])\n",
    "\n",
    "        # 取出[CLS] token對應的向量作為整個文本的向量表示\n",
    "        embeddings.append(model_output.last_hidden_state[:, 0, :].squeeze().tolist())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_bert = bert_embedding(LCLS_X_train)\n",
    "pd.DataFrame(LCLS_X_train_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCHS_X_train_bert = bert_embedding(LCHS_X_train)\n",
    "pd.DataFrame(LCHS_X_train_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCLS_X_train_bert = bert_embedding(HCLS_X_train)\n",
    "pd.DataFrame(HCLS_X_train_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCHS_X_train_bert = bert_embedding(HCHS_X_train)\n",
    "pd.DataFrame(HCHS_X_train_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_bert = bert_embedding(LCLS_X_test)\n",
    "LCHS_X_test_bert = bert_embedding(LCHS_X_test)\n",
    "HCLS_X_test_bert = bert_embedding(HCLS_X_test)\n",
    "HCHS_X_test_bert = bert_embedding(HCHS_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package loading & function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 16:06:36.647807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 16:06:36.647828: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 16:06:36.651439: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 16:06:36.941439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 16:06:37.945656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# ML\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# DL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation function\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def validation(model_name, X_train, y_train, word_vec_train):\n",
    "    cv = 10\n",
    "    \n",
    "    # MSE\n",
    "    mse_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='neg_mean_squared_error') # mse\n",
    "    mse_scores = -mse_scores # transfer to positive\n",
    "    avg_mse = mse_scores.mean()\n",
    "    \n",
    "    # MAE\n",
    "    mae_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    mae_scores = -mae_scores  # Convert to positive values\n",
    "    avg_mae = mae_scores.mean()\n",
    "    \n",
    "    # MAPE\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)  # Create custom scorer\n",
    "    mape_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring=mape_scorer)\n",
    "    mape_scores = -mape_scores  # Convert to positive values\n",
    "    avg_mape = mape_scores.mean()\n",
    "    \n",
    "    # R-squared\n",
    "    r2_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='r2')\n",
    "    avg_r2 = r2_scores.mean()\n",
    "\n",
    "    print(f\"{word_vec_train}'s MSE, MAE, MAPE, R^2: {avg_mse}, {avg_mae}, {avg_mape}, {avg_r2}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation\n",
    "def evaluation(y_test, y_pred, word_vec_test):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{word_vec_test}'s MSE, MAE, MAPE, R^2: {mse}, {mae}, {mape}, {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define EarlyStopping callback \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # 監控驗證集上的損失值\n",
    "    patience=10,  # 如果性能在10個epoch內沒有改善，則停止訓練\n",
    "    verbose=1,  \n",
    "    restore_best_weights=True  # 恢復最佳權重\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize word vector\n",
    "Data = {\n",
    "    'LCLS': {\n",
    "        'tf': LCLS_X_train_bow_df,\n",
    "        'tf-idf': LCLS_X_train_tfidf_df,\n",
    "        'd2v': LCLS_X_train_d2v,\n",
    "        'glove': LCLS_X_train_glove,\n",
    "        'bert': LCLS_X_train_bert,\n",
    "        'tf_test': LCLS_X_test_bow_df,\n",
    "        'tf-idf_test': LCLS_X_test_tfidf_df,\n",
    "        'd2v_test': LCLS_X_test_d2v, \n",
    "        'glove_test': LCLS_X_test_glove, \n",
    "        'bert_test': LCLS_X_test_bert, \n",
    "        'y_train' : LCLS_y_train,\n",
    "        'y_test' : LCLS_y_test, \n",
    "    },\n",
    "    'LCHS': {\n",
    "        'tf': LCHS_X_train_bow_df,\n",
    "        'tf-idf': LCHS_X_train_tfidf_df,\n",
    "        'd2v': LCHS_X_train_d2v,\n",
    "        'glove': LCHS_X_train_glove,\n",
    "        'bert': LCHS_X_train_bert,\n",
    "        'tf_test': LCHS_X_test_bow_df,\n",
    "        'tf-idf_test': LCHS_X_test_tfidf_df,\n",
    "        'd2v_test': LCHS_X_test_d2v,\n",
    "        'glove_test': LCHS_X_test_glove,\n",
    "        'bert_test': LCHS_X_test_bert,\n",
    "        'y_train': LCHS_y_train,\n",
    "        'y_test': LCHS_y_test,\n",
    "    },\n",
    "    'HCLS': {\n",
    "        'tf': HCLS_X_train_bow_df,\n",
    "        'tf-idf': HCLS_X_train_tfidf_df,\n",
    "        'd2v': HCLS_X_train_d2v,\n",
    "        'glove': HCLS_X_train_glove,\n",
    "        'bert': HCLS_X_train_bert,\n",
    "        'tf_test': HCLS_X_test_bow_df,\n",
    "        'tf-idf_test': HCLS_X_test_tfidf_df,\n",
    "        'd2v_test': HCLS_X_test_d2v,\n",
    "        'glove_test': HCLS_X_test_glove,\n",
    "        'bert_test': HCLS_X_test_bert,\n",
    "        'y_train': HCLS_y_train,\n",
    "        'y_test': HCLS_y_test,\n",
    "    },\n",
    "    'HCHS': {\n",
    "        'tf': HCHS_X_train_bow_df,\n",
    "        'tf-idf': HCHS_X_train_tfidf_df,\n",
    "        'd2v': HCHS_X_train_d2v,\n",
    "        'glove': HCHS_X_train_glove,\n",
    "        'bert': HCHS_X_train_bert,\n",
    "        'tf_test': HCHS_X_test_bow_df,\n",
    "        'tf-idf_test': HCHS_X_test_tfidf_df,\n",
    "        'd2v_test': HCHS_X_test_d2v,\n",
    "        'glove_test': HCHS_X_test_glove,\n",
    "        'bert_test': HCHS_X_test_bert,\n",
    "        'y_train': HCHS_y_train,\n",
    "        'y_test': HCHS_y_test,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML model\n",
    "* SVR\n",
    "* Random Forest\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_train = 'tf-idf'\n",
    "word_vec_test = 'tf-idf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_LCLS = SVR(epsilon=0.2, kernel='rbf')\n",
    "svr_LCHS = SVR(epsilon=0.2, kernel='rbf')\n",
    "svr_HCLS = SVR(epsilon=0.2, kernel='rbf')\n",
    "svr_HCHS = SVR(epsilon=0.2, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(epsilon=0.2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(epsilon=0.2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(epsilon=0.2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_LCLS.fit(Data['LCLS'][word_vec_train], LCLS_y_train)\n",
    "svr_LCHS.fit(Data['LCHS'][word_vec_train], LCHS_y_train)\n",
    "svr_HCLS.fit(Data['HCLS'][word_vec_train], HCLS_y_train)\n",
    "svr_HCHS.fit(Data['HCHS'][word_vec_train], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_LCLS, Data['LCLS'][word_vec_train], LCLS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_LCHS, Data['LCHS'][word_vec_train], LCHS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_HCLS, Data['HCLS'][word_vec_train], HCLS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_HCHS, Data['HCHS'][word_vec_train], HCHS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "svr_y_pred_LCLS = svr_LCLS.predict(Data['LCLS'][word_vec_test])\n",
    "svr_y_pred_LCHS = svr_LCHS.predict(Data['LCHS'][word_vec_test])\n",
    "svr_y_pred_HCLS = svr_HCLS.predict(Data['HCLS'][word_vec_test])\n",
    "svr_y_pred_HCHS = svr_HCHS.predict(Data['HCHS'][word_vec_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS\n",
    "evaluation(LCLS_y_test, svr_y_pred_LCLS, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCHS\n",
    "evaluation(LCHS_y_test, svr_y_pred_LCHS, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCLS\n",
    "evaluation(HCLS_y_test, svr_y_pred_HCLS, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCHS\n",
    "evaluation(HCHS_y_test, svr_y_pred_HCHS, word_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_train_rf = 'bert'\n",
    "word_vec_test_rf = 'bert_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_LCLS = RandomForestRegressor()\n",
    "rf_LCHS = RandomForestRegressor()\n",
    "rf_HCLS = RandomForestRegressor()\n",
    "rf_HCHS = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_LCLS.fit(Data['LCLS'][word_vec_train_rf], LCLS_y_train)\n",
    "rf_LCHS.fit(Data['LCHS'][word_vec_train_rf], LCHS_y_train)\n",
    "rf_HCLS.fit(Data['HCLS'][word_vec_train_rf], HCLS_y_train)\n",
    "rf_HCHS.fit(Data['HCHS'][word_vec_train_rf], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_LCLS, Data['LCLS'][word_vec_train_rf], LCLS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_LCHS, Data['LCHS'][word_vec_train_rf], LCHS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_HCLS, Data['HCLS'][word_vec_train_rf], HCLS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_HCHS, Data['HCHS'][word_vec_train_rf], HCHS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "rf_y_pred_LCLS = rf_LCLS.predict(Data['LCLS'][word_vec_test_rf])\n",
    "rf_y_pred_LCHS = rf_LCHS.predict(Data['LCHS'][word_vec_test_rf])\n",
    "rf_y_pred_HCLS = rf_HCLS.predict(Data['HCLS'][word_vec_test_rf])\n",
    "rf_y_pred_HCHS = rf_HCHS.predict(Data['HCHS'][word_vec_test_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluation(LCLS_y_test, rf_y_pred_LCLS, word_vec_test_rf)\n",
    "evaluation(LCHS_y_test, rf_y_pred_LCHS, word_vec_test_rf)\n",
    "evaluation(HCLS_y_test, rf_y_pred_HCLS, word_vec_test_rf)\n",
    "evaluation(HCHS_y_test, rf_y_pred_HCHS, word_vec_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_train_xgb = 'tf'\n",
    "word_vec_test_xgb = 'tf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_LCLS = xgb.XGBRegressor()\n",
    "xgb_LCHS = xgb.XGBRegressor()\n",
    "xgb_HCLS = xgb.XGBRegressor()\n",
    "xgb_HCHS = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_LCLS.fit(Data['LCLS'][word_vec_train_xgb], LCLS_y_train)\n",
    "xgb_LCHS.fit(Data['LCHS'][word_vec_train_xgb], LCHS_y_train)\n",
    "xgb_HCLS.fit(Data['HCLS'][word_vec_train_xgb], HCLS_y_train)\n",
    "xgb_HCHS.fit(Data['HCHS'][word_vec_train_xgb], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_LCLS, Data['LCLS'][word_vec_train_xgb], LCLS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_LCHS, Data['LCHS'][word_vec_train_xgb], LCHS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_HCLS, Data['HCLS'][word_vec_train_xgb], HCLS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_HCHS, Data['HCHS'][word_vec_train_xgb], HCHS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "xgb_y_pred_LCLS = xgb_LCLS.predict(Data['LCLS'][word_vec_test_xgb])\n",
    "xgb_y_pred_LCHS = xgb_LCHS.predict(Data['LCHS'][word_vec_test_xgb])\n",
    "xgb_y_pred_HCLS = xgb_HCLS.predict(Data['HCLS'][word_vec_test_xgb])\n",
    "xgb_y_pred_HCHS = xgb_HCHS.predict(Data['HCHS'][word_vec_test_xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS, LCHS, HCLS, HCHS\n",
    "evaluation(LCLS_y_test, xgb_y_pred_LCLS, word_vec_test_xgb)\n",
    "evaluation(LCHS_y_test, xgb_y_pred_LCHS, word_vec_test_xgb)\n",
    "evaluation(HCLS_y_test, xgb_y_pred_HCLS, word_vec_test_xgb)\n",
    "evaluation(HCHS_y_test, xgb_y_pred_HCHS, word_vec_test_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch(Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: TF-IDF DataFrame and rating(y) transform to PyTorch tensor\n",
    "X = torch.tensor(LCLS_X_train_tfidf_df.values, dtype=torch.float32)\n",
    "y = torch.tensor(LCLS_y_train.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: contruct Pytorch dataloader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(X, y)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3: define model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc3 = nn.Linear(hidden_size, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "input_size = LCLS_X_train_tfidf_df.shape[1]  # 輸入特徵的維度\n",
    "hidden_size = 128  # 隱藏層的神經元數量\n",
    "output_size = 1  # 輸出的評分值\n",
    "\n",
    "model = MLPModel(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: training\n",
    "criterion = nn.MSELoss()  # 使用均方誤差損失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = 'LCLS'\n",
    "word_vec_train = 'tf'\n",
    "word_vec_test = 'tf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[sub_dataset][word_vec_train]\n",
    "subdata_y_train = Data[sub_dataset]['y_train']\n",
    "subdata_X_test_embedding = Data[sub_dataset][word_vec_test]\n",
    "subdata_y_test = Data[sub_dataset]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:18<00:00,  7.57s/trial, best loss: 0.446944510988743]\n",
      "Best hyperparameters: {'activation': 0, 'batch_size': 128.0, 'dropout': 0.37711638606751985, 'epochs': 60.0, 'optimizer': 2, 'units': 96.0, 'units_h': 32.0}\n",
      "Best MSE: 0.446944510988743\n"
     ]
    }
   ],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def MLP_model_para(X_train_embedding, params):\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(units=int(params['units']), input_dim=X_train_embedding.shape[1], activation=params['activation']))\n",
    "    nn_model.add(Dropout(params['dropout']))\n",
    "    nn_model.add(Dense(units=int(params['units_h']), activation=params['activation']))\n",
    "    nn_model.add(Dropout(params['dropout']))\n",
    "    nn_model.add(Dense(1, activation=params['activation']))  # Use linear for regression \n",
    "    \n",
    "    nn_model.compile(loss='mean_squared_error', optimizer=params['optimizer'])\n",
    "    return nn_model\n",
    "\n",
    "def objective(params):\n",
    "    model = MLP_model_para(subdata_X_train_embedding.to_numpy(), params)\n",
    "\n",
    "    model.fit(subdata_X_train_embedding.to_numpy(), subdata_y_train.to_numpy(), epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(subdata_X_test_embedding.to_numpy())\n",
    "    mse = mean_squared_error(subdata_y_test, y_pred)\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'units': hp.quniform('units', 32, 256, 32),\n",
    "    'units_h': hp.quniform('units_h', 32, 256, 32),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
    "    'dropout': hp.uniform('dropout', 0, 1),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()  # Create a trials object to track the progress\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Can also access the results and losses from the trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 0,\n",
       " 'batch_size': 128.0,\n",
       " 'dropout': 0.37711638606751985,\n",
       " 'epochs': 60.0,\n",
       " 'optimizer': 2,\n",
       " 'units': 96.0,\n",
       " 'units_h': 32.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "activation = 'relu'\n",
    "batch_size = int(best['batch_size'])\n",
    "dropout = best['dropout']\n",
    "epochs = 70\n",
    "optimizer = 'rmsprop'\n",
    "units = 256\n",
    "units_h = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(X_train_embedding, y_train):\n",
    "  # define model\n",
    "  nn_model = Sequential()\n",
    "  # Input - Layer\n",
    "  nn_model.add(Dense(units=128, input_dim=X_train_embedding.shape[1], activation='relu'))\n",
    "  # Hidden - Layers\n",
    "  nn_model.add(Dropout(0.3))\n",
    "  nn_model.add(Dense(units=64, activation='relu'))\n",
    "  nn_model.add(Dropout(0.3))\n",
    "  # Output- Layer\n",
    "  nn_model.add(Dense(1, activation='relu'))\n",
    "\n",
    "  nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    y = y.to_numpy()  # Convert Series to NumPy array\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = MLP_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores,  mae_scores, mape_scores,  r2_scores = cross_val_metrics(subdata_X_train_embedding, subdata_y_train)\n",
    "\n",
    "print(f\"{sub_dataset}'s MSE, MAE, MAPE, R^2: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/Code/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:73: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.4360 - val_loss: 1.5137\n",
      "Epoch 2/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7270 - val_loss: 1.0347\n",
      "Epoch 3/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2027 - val_loss: 0.8398\n",
      "Epoch 4/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9141 - val_loss: 0.7638\n",
      "Epoch 5/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8721 - val_loss: 0.7179\n",
      "Epoch 6/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7694 - val_loss: 0.6920\n",
      "Epoch 7/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6961 - val_loss: 0.6777\n",
      "Epoch 8/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6560 - val_loss: 0.7268\n",
      "Epoch 9/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6135 - val_loss: 0.6583\n",
      "Epoch 10/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5890 - val_loss: 0.6643\n",
      "Epoch 11/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5491 - val_loss: 0.6448\n",
      "Epoch 12/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5230 - val_loss: 0.6621\n",
      "Epoch 13/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5114 - val_loss: 0.6284\n",
      "Epoch 14/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4854 - val_loss: 0.6260\n",
      "Epoch 15/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4570 - val_loss: 0.6353\n",
      "Epoch 16/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4370 - val_loss: 0.6192\n",
      "Epoch 17/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4288 - val_loss: 0.6025\n",
      "Epoch 18/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4288 - val_loss: 0.6157\n",
      "Epoch 19/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4168 - val_loss: 0.5993\n",
      "Epoch 20/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3819 - val_loss: 0.6086\n",
      "Epoch 21/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3689 - val_loss: 0.5797\n",
      "Epoch 22/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3781 - val_loss: 0.5954\n",
      "Epoch 23/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3684 - val_loss: 0.5855\n",
      "Epoch 24/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3603 - val_loss: 0.5666\n",
      "Epoch 25/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3505 - val_loss: 0.5540\n",
      "Epoch 26/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3392 - val_loss: 0.5606\n",
      "Epoch 27/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3294 - val_loss: 0.5600\n",
      "Epoch 28/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3128 - val_loss: 0.5514\n",
      "Epoch 29/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3166 - val_loss: 0.5594\n",
      "Epoch 30/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3128 - val_loss: 0.5533\n",
      "Epoch 31/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2918 - val_loss: 0.5468\n",
      "Epoch 32/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2965 - val_loss: 0.5580\n",
      "Epoch 33/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2993 - val_loss: 0.5408\n",
      "Epoch 34/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2817 - val_loss: 0.5459\n",
      "Epoch 35/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2864 - val_loss: 0.5315\n",
      "Epoch 36/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2771 - val_loss: 0.5346\n",
      "Epoch 37/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2733 - val_loss: 0.5288\n",
      "Epoch 38/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2723 - val_loss: 0.5332\n",
      "Epoch 39/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2871 - val_loss: 0.5329\n",
      "Epoch 40/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2616 - val_loss: 0.5233\n",
      "Epoch 41/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2608 - val_loss: 0.5303\n",
      "Epoch 42/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2529 - val_loss: 0.5307\n",
      "Epoch 43/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2510 - val_loss: 0.5268\n",
      "Epoch 44/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2407 - val_loss: 0.5191\n",
      "Epoch 45/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2449 - val_loss: 0.5147\n",
      "Epoch 46/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2410 - val_loss: 0.5252\n",
      "Epoch 47/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2424 - val_loss: 0.5130\n",
      "Epoch 48/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2338 - val_loss: 0.5237\n",
      "Epoch 49/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2302 - val_loss: 0.5072\n",
      "Epoch 50/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2274 - val_loss: 0.5218\n",
      "Epoch 51/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2284 - val_loss: 0.5167\n",
      "Epoch 52/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2328 - val_loss: 0.5175\n",
      "Epoch 53/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2211 - val_loss: 0.5193\n",
      "Epoch 54/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2162 - val_loss: 0.5138\n",
      "Epoch 55/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2133 - val_loss: 0.5125\n",
      "Epoch 56/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2068 - val_loss: 0.5170\n",
      "Epoch 57/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2008 - val_loss: 0.5131\n",
      "Epoch 58/70\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2021 - val_loss: 0.5083\n",
      "Epoch 59/70\n",
      "\u001b[1m44/70\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1891 Restoring model weights from the end of the best epoch: 49.\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1916 - val_loss: 0.5150\n",
      "Epoch 59: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e3634e78250>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "mlp_model = MLP_model(subdata_X_train_embedding, subdata_y_train)\n",
    "mlp_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "tf_test's MSE, MAE, MAPE, R^2: 0.49181434259683304, 0.5063726552646054, 0.18429567959602236, 0.7896580633960381\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "mlp_y_pred = mlp_model.predict(subdata_X_test_embedding)\n",
    "mlp_y_pred = mlp_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(subdata_y_test, mlp_y_pred, word_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset_cnn = 'HCHS'\n",
    "word_vec_train_cnn = 'glove'\n",
    "word_vec_test_cnn = 'glove_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[sub_dataset_cnn][word_vec_train_cnn]\n",
    "subdata_y_train = Data[sub_dataset_cnn]['y_train']\n",
    "subdata_X_test_embedding = Data[sub_dataset_cnn][word_vec_test_cnn]\n",
    "subdata_y_test = Data[sub_dataset_cnn]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# fill in missing values with the mean of the column\n",
    "subdata_X_train_embedding.fillna(subdata_X_train_embedding.mean(), inplace=True)\n",
    "subdata_y_train.fillna(subdata_y_train.mean(), inplace=True)\n",
    "subdata_X_test_embedding.fillna(subdata_X_test_embedding.mean(), inplace=True)\n",
    "subdata_y_test.fillna(subdata_y_test.mean(), inplace=True)\n",
    "\n",
    "def CNN_model_para(X_train_shape, params):\n",
    "    cnn_model = Sequential()\n",
    "    # 1st Conv1D + MaxPooling1D layer  \n",
    "    cnn_model.add(Conv1D(filters=int(params['filters']), kernel_size=int(params['kernel_size']), activation='relu', padding='same', input_shape=(X_train_shape[1], 1)))\n",
    "    cnn_model.add(Conv1D(filters=int(params['filters_1']), kernel_size=int(params['kernel_size']), activation='relu', padding='same'))\n",
    "    cnn_model.add(MaxPooling1D(int(params['pool_size']), padding='same'))\n",
    "    # Flatten\n",
    "    cnn_model.add(Flatten())\n",
    "    # Fully connected layers\n",
    "    cnn_model.add(Dropout(params['dropout']))\n",
    "    cnn_model.add(Dense(units=int(params['dense_units']), activation='relu'))\n",
    "    # # Output layer for regression\n",
    "    cnn_model.add(Dense(1, activation='linear'))\n",
    "    cnn_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return cnn_model\n",
    "\n",
    "def objective(params):\n",
    "    cnn_model = CNN_model_para(subdata_X_train_embedding.shape, params)\n",
    "    cnn_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    y_pred = cnn_model.predict(subdata_X_test_embedding)\n",
    "    mse = mean_squared_error(subdata_y_test, y_pred)\n",
    "    # clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'filters': hp.quniform('filters', 32, 256, 32),\n",
    "    'filters_1': hp.quniform('filters_1', 32, 256, 32),\n",
    "    'kernel_size': hp.choice('kernel_size', [7, 9, 11, 13]),\n",
    "    'pool_size': hp.choice('pool_size', [2, 3, 5]),\n",
    "    'dropout': hp.uniform('dropout', 0, 1),\n",
    "    'dense_units': hp.quniform('dense_units', 32, 256, 32),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Can also access the results and losses from th\n",
    "# e trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters: {'batch_size': 32.0, 'dense_units': 192.0, 'dropout': 0.20990214723448342, 'epochs': 2, 'filters': 224.0, 'filters_1': 96.0, 'kernel_size': 3, 'optimizer': 2, 'pool_size': 1}\n",
    "batch_size = int(best['batch_size'])\n",
    "dense_units = int(best['dense_units'])\n",
    "dropout = best['dropout']\n",
    "epochs = 70\n",
    "filters = int(best['filters'])\n",
    "filters_1 = int(best['filters_1'])\n",
    "kernel_size = 9\n",
    "optimizer = 'rmsprop'\n",
    "pool_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(X_train_embedding, y_train):\n",
    "    # define model\n",
    "    cnn_model = Sequential()\n",
    "    \n",
    "    # Conv1D 2 layer + MaxPooling1D layer\n",
    "    cnn_model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=(X_train_embedding.shape[1], 1)))\n",
    "    cnn_model.add(Conv1D(filters=filters_1, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size, padding='same'))\n",
    "    \n",
    "    # 2nd Conv1D + MaxPooling1D layer\n",
    "    # cnn_model.add(Conv1D(filters=int(filters*2), kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    # cnn_model.add(MaxPooling1D(pool_size, padding='same'))\n",
    "    cnn_model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers\n",
    "    cnn_model.add(Dropout(dropout))\n",
    "    cnn_model.add(Dense(units=dense_units, activation='relu'))\n",
    "    \n",
    "    # Output layer for regression\n",
    "    cnn_model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    cnn_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    y = y.to_numpy()  # Convert Series to NumPy array\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = CNN_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores, mae_scores, mape_scores, r2_scores = cross_val_metrics(subdata_X_train_embedding, subdata_y_train)\n",
    "print(f\"{sub_dataset_cnn}'s MSE, MAE, MAPE, R^2: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "cnn_model = CNN_model(subdata_X_train_embedding, subdata_y_train)\n",
    "cnn_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "cnn_y_pred = cnn_model.predict(subdata_X_test_embedding)\n",
    "cnn_y_pred = cnn_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(subdata_y_test, cnn_y_pred, word_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset_lstm = 'HCHS'\n",
    "word_vec_train_lstm = 'glove'\n",
    "word_vec_test_lstm = 'glove_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[sub_dataset_lstm][word_vec_train_lstm]\n",
    "subdata_y_train = Data[sub_dataset_lstm]['y_train']\n",
    "subdata_X_test_embedding = Data[sub_dataset_lstm][word_vec_test_lstm]\n",
    "subdata_y_test = Data[sub_dataset_lstm]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape vector to 3D\n",
    "subdata_X_train_embedding = subdata_X_train_embedding.to_numpy().reshape(subdata_X_train_embedding.shape[0], 1, subdata_X_train_embedding.shape[1])\n",
    "subdata_X_test_embedding = subdata_X_test_embedding.to_numpy().reshape(subdata_X_test_embedding.shape[0], 1, subdata_X_test_embedding.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:56<00:00, 11.93s/trial, best loss: 0.5471184613798864]\n",
      "Best hyperparameters: {'batch_size': 32.0, 'dropout': 0.03369545217380508, 'epochs': 70.0, 'optimizer': 0, 'return_sequences': 0, 'units': 160.0}\n",
      "Best MSE: 0.5471184613798864\n"
     ]
    }
   ],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def lstm_model_para(X_train_embedding, params):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=int(params['units']), return_sequences=params['return_sequences'], input_shape=(X_train_embedding.shape[1], X_train_embedding.shape[2])))\n",
    "    lstm_model.add(Dropout(params['dropout']))\n",
    "    lstm_model.add(Dense(units=1, activation='linear'))\n",
    "    lstm_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return lstm_model\n",
    "\n",
    "def objective(params):\n",
    "    lstm_model = lstm_model_para(subdata_X_train_embedding, params)\n",
    "\n",
    "    lstm_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = lstm_model.predict(subdata_X_test_embedding)\n",
    "    mse = mean_squared_error(subdata_y_test, y_pred.flatten())\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'units': hp.quniform('units', 16, 256, 32),\n",
    "    'return_sequences': hp.choice('return_sequences', [True, False]),\n",
    "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Also access the results and losses from the trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best['batch_size'])\n",
    "units = int(best['units'])\n",
    "dropout = best['dropout']\n",
    "epochs = 100\n",
    "optimizer = 'adam'\n",
    "return_sequences = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train_embedding, y_train): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=return_sequences, input_shape=(X_train_embedding.shape[1], X_train_embedding.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    # rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    # corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    # # X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    # y = y.values.ravel()  # Convert Series to NumPy array\n",
    "    y = y.reset_index(drop=True)  # reset y index\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = LSTM_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores, mae_scores, mape_scores, r2_scores = cross_val_metrics(subdata_X_train_embedding, subdata_y_train)\n",
    "print(f\"{sub_dataset_lstm}'MSE, MAE, MAPE, CC: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "lstm_model = LSTM_model(subdata_X_train_embedding, subdata_y_train)\n",
    "lstm_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "lstm_y_pred = lstm_model.predict(subdata_X_test_embedding)\n",
    "lstm_y_pred = lstm_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(subdata_y_test, lstm_y_pred, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save LSTM model and architecture to single file\n",
    "lstm_model.save(\"Model/HCHS_lstm_model_longformer.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
