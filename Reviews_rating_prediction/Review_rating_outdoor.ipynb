{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package & Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/Code/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hao/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hao/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza.download('en')\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_locID</th>\n",
       "      <th>Hotel_geoID_x</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Date_of_stay</th>\n",
       "      <th>Trip_type</th>\n",
       "      <th>Review_helpful_votes</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Contributions</th>\n",
       "      <th>Reviewer_helpful_vote</th>\n",
       "      <th>...</th>\n",
       "      <th>Hotel_rating</th>\n",
       "      <th>Location_score</th>\n",
       "      <th>Resaurant_count</th>\n",
       "      <th>Attractions_count</th>\n",
       "      <th>Hotel_styles</th>\n",
       "      <th>Popular_mentions</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Review</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Unreliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>913352673</td>\n",
       "      <td>23-Aug</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>Decent for the Price, but Lacks Attention to D...</td>\n",
       "      <td>23-Aug</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>508</td>\n",
       "      <td>I recently got to stay at the SPOT X Hotel, pa...</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>833765796</td>\n",
       "      <td>22-Feb</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>2</td>\n",
       "      <td>Best Choice ???ÂÂ» Stay in Orlando</td>\n",
       "      <td>22-Apr</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>45</td>\n",
       "      <td>The perfect stay for a smart coice. Brand New,...</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>906461005</td>\n",
       "      <td>23-Jul</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>Gem of a hotel offering great value!</td>\n",
       "      <td>23-Jul</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>143</td>\n",
       "      <td>Had a restful stay in 2023 from July 27-30, an...</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>898109791</td>\n",
       "      <td>23-Jun</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Stay!!</td>\n",
       "      <td>23-Jun</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>47</td>\n",
       "      <td>Everything about the hotel was great! Excellen...</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>891583554</td>\n",
       "      <td>22-Nov</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>New for now</td>\n",
       "      <td>23-May</td>\n",
       "      <td>149</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>63</td>\n",
       "      <td>We did stay there for a week and the beds are ...</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32287</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>417505916</td>\n",
       "      <td>16-Aug</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Worse hotel ever</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>209</td>\n",
       "      <td>I stayed the hotel because of the low price. B...</td>\n",
       "      <td>-0.9830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32288</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>415134854</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>WORST HOTEL I have EVER stayed at</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>130</td>\n",
       "      <td>I promise you I will NEVER ever use this hotel...</td>\n",
       "      <td>0.6428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32289</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>413963936</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Will NEVER STAY here again</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>49</td>\n",
       "      <td>First off roaches in the room live and dead. T...</td>\n",
       "      <td>-0.9001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32290</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>395650617</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Not horrible but far from doable for more than...</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>145</td>\n",
       "      <td>Walking in it took forever to get checked in. ...</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32291</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>394406121</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Overnight stay when moving daughter.</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>102</td>\n",
       "      <td>Although the price was very reasonable at abou...</td>\n",
       "      <td>0.6093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32292 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hotel_locID  Hotel_geoID_x  Review_id Date_of_stay  \\\n",
       "0         23144733          34515  913352673       23-Aug   \n",
       "1         23144733          34515  833765796       22-Feb   \n",
       "2         23144733          34515  906461005       23-Jul   \n",
       "3         23144733          34515  898109791       23-Jun   \n",
       "4         23144733          34515  891583554       22-Nov   \n",
       "...            ...            ...        ...          ...   \n",
       "32287      2290607          34515  417505916       16-Aug   \n",
       "32288      2290607          34515  415134854       16-Sep   \n",
       "32289      2290607          34515  413963936       16-Sep   \n",
       "32290      2290607          34515  395650617       16-Jul   \n",
       "32291      2290607          34515  394406121       16-Jul   \n",
       "\n",
       "                  Trip_type  Review_helpful_votes  \\\n",
       "0             Traveled solo                     0   \n",
       "1      Traveled on business                     2   \n",
       "2      Traveled as a couple                     0   \n",
       "3      Traveled on business                     1   \n",
       "4      Traveled on business                     0   \n",
       "...                     ...                   ...   \n",
       "32287  Traveled with family                     0   \n",
       "32288  Traveled as a couple                     0   \n",
       "32289  Traveled with family                     0   \n",
       "32290                   NaN                     0   \n",
       "32291  Traveled with family                     0   \n",
       "\n",
       "                                                   Title Review_Date  \\\n",
       "0      Decent for the Price, but Lacks Attention to D...      23-Aug   \n",
       "1                    Best Choice ???ÂÂ» Stay in Orlando      22-Apr   \n",
       "2                   Gem of a hotel offering great value!      23-Jul   \n",
       "3                                       Excellent Stay!!      23-Jun   \n",
       "4                                            New for now      23-May   \n",
       "...                                                  ...         ...   \n",
       "32287                                   Worse hotel ever      16-Sep   \n",
       "32288                  WORST HOTEL I have EVER stayed at      16-Sep   \n",
       "32289                         Will NEVER STAY here again      16-Sep   \n",
       "32290  Not horrible but far from doable for more than...      16-Jul   \n",
       "32291               Overnight stay when moving daughter.      16-Jul   \n",
       "\n",
       "       Reviewer_Contributions  Reviewer_helpful_vote  ...  Hotel_rating  \\\n",
       "0                           8                      0  ...           4.5   \n",
       "1                           2                      4  ...           4.5   \n",
       "2                          19                     29  ...           4.5   \n",
       "3                           0                      0  ...           4.5   \n",
       "4                         149                     62  ...           4.5   \n",
       "...                       ...                    ...  ...           ...   \n",
       "32287                       0                      2  ...           1.5   \n",
       "32288                       0                      2  ...           1.5   \n",
       "32289                      13                      2  ...           1.5   \n",
       "32290                       0                      0  ...           1.5   \n",
       "32291                      53                     19  ...           1.5   \n",
       "\n",
       "       Location_score  Resaurant_count  Attractions_count  Hotel_styles  \\\n",
       "0                67.0             52.0                4.0            []   \n",
       "1                67.0             52.0                4.0            []   \n",
       "2                67.0             52.0                4.0            []   \n",
       "3                67.0             52.0                4.0            []   \n",
       "4                67.0             52.0                4.0            []   \n",
       "...               ...              ...                ...           ...   \n",
       "32287             NaN              NaN                NaN    ['Budget']   \n",
       "32288             NaN              NaN                NaN    ['Budget']   \n",
       "32289             NaN              NaN                NaN    ['Budget']   \n",
       "32290             NaN              NaN                NaN    ['Budget']   \n",
       "32291             NaN              NaN                NaN    ['Budget']   \n",
       "\n",
       "                                        Popular_mentions  WordCount  \\\n",
       "0      ['outlet mall', 'excellent cost benefit', 'exc...        508   \n",
       "1      ['outlet mall', 'excellent cost benefit', 'exc...         45   \n",
       "2      ['outlet mall', 'excellent cost benefit', 'exc...        143   \n",
       "3      ['outlet mall', 'excellent cost benefit', 'exc...         47   \n",
       "4      ['outlet mall', 'excellent cost benefit', 'exc...         63   \n",
       "...                                                  ...        ...   \n",
       "32287                                                 []        209   \n",
       "32288                                                 []        130   \n",
       "32289                                                 []         49   \n",
       "32290                                                 []        145   \n",
       "32291                                                 []        102   \n",
       "\n",
       "                                                  Review Compound_Score  \\\n",
       "0      I recently got to stay at the SPOT X Hotel, pa...         0.9967   \n",
       "1      The perfect stay for a smart coice. Brand New,...         0.9584   \n",
       "2      Had a restful stay in 2023 from July 27-30, an...         0.9902   \n",
       "3      Everything about the hotel was great! Excellen...         0.9390   \n",
       "4      We did stay there for a week and the beds are ...         0.2363   \n",
       "...                                                  ...            ...   \n",
       "32287  I stayed the hotel because of the low price. B...        -0.9830   \n",
       "32288  I promise you I will NEVER ever use this hotel...         0.6428   \n",
       "32289  First off roaches in the room live and dead. T...        -0.9001   \n",
       "32290  Walking in it took forever to get checked in. ...         0.8705   \n",
       "32291  Although the price was very reasonable at abou...         0.6093   \n",
       "\n",
       "       Unreliable  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "32287           0  \n",
       "32288           0  \n",
       "32289           0  \n",
       "32290           0  \n",
       "32291           0  \n",
       "\n",
       "[32292 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Final_Datasets/TA_combined_df_Outdoor_tourism_type_VADER_final.csv', encoding='latin1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3. , 4. , 3.5, 2. , 2.5, 5. , nan, 4.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Hotel_star'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview & Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER\n",
    "def calculate_compound_score(review):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(review)['compound']\n",
    "\n",
    "# Unreliable tag\n",
    "def calculate_unreliable(row):\n",
    "    compound_score = row['Compound_Score']\n",
    "    rating = row['Review_Rating']\n",
    "    \n",
    "    if (compound_score < -0.49 and rating >= 3):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Compound_Score'] = df['Review'].apply(calculate_compound_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unreliable\n",
      "0    31714\n",
      "1      578\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df['Unreliable'] = df.apply(calculate_unreliable, axis=1)\n",
    "print(df['Unreliable'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Stanza_Score'] = df['Review'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51     Always great but hotel was embarrassed this ti...\n",
       "65     The Hyatt Regency is supposed to be the luxury...\n",
       "100    Senior Hyatt Regency Management needs to take ...\n",
       "111    We stayed @ Hyatt Grand Cypress resort from Mo...\n",
       "114    The room was a decent size with a big TV. The ...\n",
       "121    Used / Dirty cups were spread out across the h...\n",
       "129    Charged $30 for package had shipped to us Dail...\n",
       "332    Excellent service from the receptionist Talli....\n",
       "344    The hottub was broken, overcharged by adding 2...\n",
       "360    The food for my large group was not the greate...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unreliable reviews example\n",
    "unreliable_reviews = df[df['Unreliable'] == 1]['Review']\n",
    "unreliable_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reviews rating distributions\n",
    "rating_counts = df['Review_Rating'].value_counts()\n",
    "rating_counts = rating_counts.sort_index()\n",
    "plt.figure(figsize=(8, 5))  # size\n",
    "plt.bar(rating_counts.index, rating_counts.values)\n",
    "plt.xlabel('Review_Rating')  \n",
    "plt.ylabel('Count')  \n",
    "plt.title('Distribution of Ratings')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracting(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'bout\", \"about\", text)\n",
    "    text = re.sub(r\"\\'til\", \"until\", text)\n",
    "    return text\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(tokens):\n",
    "    texts = [i for i in tokens if i not in stopwords_list]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def lemmatization(tokens):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "  return lemmatized_tokens\n",
    "\n",
    "\n",
    "def word_preprocess(df, column_name):\n",
    "  #lowercase\n",
    "  df[column_name] = df[column_name].apply(lambda x: str(x).lower())\n",
    "\n",
    "  #decontracting\n",
    "  df[column_name] = df[column_name].apply(decontracting)\n",
    "\n",
    "  #remove tags, punctuations, numbers\n",
    "  df[column_name] = df[column_name].apply(lambda x: re.sub('[^a-zA-Z!]', ' ', x))\n",
    "\n",
    "  #tokenization\n",
    "  import nltk\n",
    "  nltk.download('punkt')\n",
    "  df[column_name] = df[column_name].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "  #remove stopwords\n",
    "  df[column_name] = df[column_name].apply(remove_stopwords)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed_df = word_preprocess(df,'Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [recently, got, stay, spot, x, hotel, part, re...\n",
       "1        [perfect, stay, smart, coice, brand, new, exce...\n",
       "2        [restful, stay, july, wonderful, experience, s...\n",
       "3        [everything, hotel, great, !, excellent, rooms...\n",
       "4        [stay, week, beds, comfortable, clean, room, l...\n",
       "                               ...                        \n",
       "32287    [stayed, hotel, low, price, take, long, realiz...\n",
       "32288    [promise, never, ever, use, hotel, first, take...\n",
       "32289    [first, roaches, room, live, dead, mold, bathr...\n",
       "32290    [walking, took, forever, get, checked, sure, k...\n",
       "32291    [although, price, reasonable, including, tax, ...\n",
       "Name: Review, Length: 32292, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nan row\n",
    "text_preprocessed_df = text_preprocessed_df.dropna(subset=['Hotel_star', 'Review_Rating', 'Review', 'Reviewer_Contributions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3. , 4. , 3.5, 2. , 2.5, 5. , 4.5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df['Hotel_star'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8147882268485285"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df['Review_Rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating counts:\n",
      " Review_Rating\n",
      "5    15804\n",
      "4     4850\n",
      "1     4566\n",
      "3     3070\n",
      "2     2356\n",
      "Name: count, dtype: int64\n",
      "Rating proportions:\n",
      " Review_Rating\n",
      "5    0.515695\n",
      "4    0.158259\n",
      "1    0.148992\n",
      "3    0.100176\n",
      "2    0.076878\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 用於描述性統計\n",
    "rating_counts = text_preprocessed_df['Review_Rating'].value_counts()\n",
    "rating_proportions = text_preprocessed_df['Review_Rating'].value_counts(normalize=True)\n",
    "\n",
    "# 打印計數和比例\n",
    "print(\"Rating counts:\\n\", rating_counts)\n",
    "print(\"Rating proportions:\\n\", rating_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets to sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_locID</th>\n",
       "      <th>Hotel_geoID_x</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Date_of_stay</th>\n",
       "      <th>Trip_type</th>\n",
       "      <th>Review_helpful_votes</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Contributions</th>\n",
       "      <th>Reviewer_helpful_vote</th>\n",
       "      <th>...</th>\n",
       "      <th>Hotel_rating</th>\n",
       "      <th>Location_score</th>\n",
       "      <th>Resaurant_count</th>\n",
       "      <th>Attractions_count</th>\n",
       "      <th>Hotel_styles</th>\n",
       "      <th>Popular_mentions</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Review</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Unreliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>913352673</td>\n",
       "      <td>23-Aug</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>Decent for the Price, but Lacks Attention to D...</td>\n",
       "      <td>23-Aug</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>508</td>\n",
       "      <td>[recently, got, stay, spot, x, hotel, part, re...</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>833765796</td>\n",
       "      <td>22-Feb</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>2</td>\n",
       "      <td>Best Choice ???ÂÂ» Stay in Orlando</td>\n",
       "      <td>22-Apr</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>45</td>\n",
       "      <td>[perfect, stay, smart, coice, brand, new, exce...</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>906461005</td>\n",
       "      <td>23-Jul</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>Gem of a hotel offering great value!</td>\n",
       "      <td>23-Jul</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>143</td>\n",
       "      <td>[restful, stay, july, wonderful, experience, s...</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>898109791</td>\n",
       "      <td>23-Jun</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Stay!!</td>\n",
       "      <td>23-Jun</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>47</td>\n",
       "      <td>[everything, hotel, great, !, excellent, rooms...</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>891583554</td>\n",
       "      <td>22-Nov</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>New for now</td>\n",
       "      <td>23-May</td>\n",
       "      <td>149</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>63</td>\n",
       "      <td>[stay, week, beds, comfortable, clean, room, l...</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32287</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>417505916</td>\n",
       "      <td>16-Aug</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Worse hotel ever</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>209</td>\n",
       "      <td>[stayed, hotel, low, price, take, long, realiz...</td>\n",
       "      <td>-0.9830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32288</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>415134854</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>WORST HOTEL I have EVER stayed at</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>130</td>\n",
       "      <td>[promise, never, ever, use, hotel, first, take...</td>\n",
       "      <td>0.6428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32289</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>413963936</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Will NEVER STAY here again</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>49</td>\n",
       "      <td>[first, roaches, room, live, dead, mold, bathr...</td>\n",
       "      <td>-0.9001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32290</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>395650617</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Not horrible but far from doable for more than...</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>145</td>\n",
       "      <td>[walking, took, forever, get, checked, sure, k...</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32291</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>394406121</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Overnight stay when moving daughter.</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>102</td>\n",
       "      <td>[although, price, reasonable, including, tax, ...</td>\n",
       "      <td>0.6093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30646 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hotel_locID  Hotel_geoID_x  Review_id Date_of_stay  \\\n",
       "0         23144733          34515  913352673       23-Aug   \n",
       "1         23144733          34515  833765796       22-Feb   \n",
       "2         23144733          34515  906461005       23-Jul   \n",
       "3         23144733          34515  898109791       23-Jun   \n",
       "4         23144733          34515  891583554       22-Nov   \n",
       "...            ...            ...        ...          ...   \n",
       "32287      2290607          34515  417505916       16-Aug   \n",
       "32288      2290607          34515  415134854       16-Sep   \n",
       "32289      2290607          34515  413963936       16-Sep   \n",
       "32290      2290607          34515  395650617       16-Jul   \n",
       "32291      2290607          34515  394406121       16-Jul   \n",
       "\n",
       "                  Trip_type  Review_helpful_votes  \\\n",
       "0             Traveled solo                     0   \n",
       "1      Traveled on business                     2   \n",
       "2      Traveled as a couple                     0   \n",
       "3      Traveled on business                     1   \n",
       "4      Traveled on business                     0   \n",
       "...                     ...                   ...   \n",
       "32287  Traveled with family                     0   \n",
       "32288  Traveled as a couple                     0   \n",
       "32289  Traveled with family                     0   \n",
       "32290                   NaN                     0   \n",
       "32291  Traveled with family                     0   \n",
       "\n",
       "                                                   Title Review_Date  \\\n",
       "0      Decent for the Price, but Lacks Attention to D...      23-Aug   \n",
       "1                    Best Choice ???ÂÂ» Stay in Orlando      22-Apr   \n",
       "2                   Gem of a hotel offering great value!      23-Jul   \n",
       "3                                       Excellent Stay!!      23-Jun   \n",
       "4                                            New for now      23-May   \n",
       "...                                                  ...         ...   \n",
       "32287                                   Worse hotel ever      16-Sep   \n",
       "32288                  WORST HOTEL I have EVER stayed at      16-Sep   \n",
       "32289                         Will NEVER STAY here again      16-Sep   \n",
       "32290  Not horrible but far from doable for more than...      16-Jul   \n",
       "32291               Overnight stay when moving daughter.      16-Jul   \n",
       "\n",
       "       Reviewer_Contributions  Reviewer_helpful_vote  ...  Hotel_rating  \\\n",
       "0                           8                      0  ...           4.5   \n",
       "1                           2                      4  ...           4.5   \n",
       "2                          19                     29  ...           4.5   \n",
       "3                           0                      0  ...           4.5   \n",
       "4                         149                     62  ...           4.5   \n",
       "...                       ...                    ...  ...           ...   \n",
       "32287                       0                      2  ...           1.5   \n",
       "32288                       0                      2  ...           1.5   \n",
       "32289                      13                      2  ...           1.5   \n",
       "32290                       0                      0  ...           1.5   \n",
       "32291                      53                     19  ...           1.5   \n",
       "\n",
       "       Location_score  Resaurant_count  Attractions_count  Hotel_styles  \\\n",
       "0                67.0             52.0                4.0            []   \n",
       "1                67.0             52.0                4.0            []   \n",
       "2                67.0             52.0                4.0            []   \n",
       "3                67.0             52.0                4.0            []   \n",
       "4                67.0             52.0                4.0            []   \n",
       "...               ...              ...                ...           ...   \n",
       "32287             NaN              NaN                NaN    ['Budget']   \n",
       "32288             NaN              NaN                NaN    ['Budget']   \n",
       "32289             NaN              NaN                NaN    ['Budget']   \n",
       "32290             NaN              NaN                NaN    ['Budget']   \n",
       "32291             NaN              NaN                NaN    ['Budget']   \n",
       "\n",
       "                                        Popular_mentions  WordCount  \\\n",
       "0      ['outlet mall', 'excellent cost benefit', 'exc...        508   \n",
       "1      ['outlet mall', 'excellent cost benefit', 'exc...         45   \n",
       "2      ['outlet mall', 'excellent cost benefit', 'exc...        143   \n",
       "3      ['outlet mall', 'excellent cost benefit', 'exc...         47   \n",
       "4      ['outlet mall', 'excellent cost benefit', 'exc...         63   \n",
       "...                                                  ...        ...   \n",
       "32287                                                 []        209   \n",
       "32288                                                 []        130   \n",
       "32289                                                 []         49   \n",
       "32290                                                 []        145   \n",
       "32291                                                 []        102   \n",
       "\n",
       "                                                  Review Compound_Score  \\\n",
       "0      [recently, got, stay, spot, x, hotel, part, re...         0.9967   \n",
       "1      [perfect, stay, smart, coice, brand, new, exce...         0.9584   \n",
       "2      [restful, stay, july, wonderful, experience, s...         0.9902   \n",
       "3      [everything, hotel, great, !, excellent, rooms...         0.9390   \n",
       "4      [stay, week, beds, comfortable, clean, room, l...         0.2363   \n",
       "...                                                  ...            ...   \n",
       "32287  [stayed, hotel, low, price, take, long, realiz...        -0.9830   \n",
       "32288  [promise, never, ever, use, hotel, first, take...         0.6428   \n",
       "32289  [first, roaches, room, live, dead, mold, bathr...        -0.9001   \n",
       "32290  [walking, took, forever, get, checked, sure, k...         0.8705   \n",
       "32291  [although, price, reasonable, including, tax, ...         0.6093   \n",
       "\n",
       "       Unreliable  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "32287           0  \n",
       "32288           0  \n",
       "32289           0  \n",
       "32290           0  \n",
       "32291           0  \n",
       "\n",
       "[30646 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if interaction exists\n",
    "def contributions_range(value):\n",
    "    if value <= 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def hotel_star_range(value):\n",
    "    if value <= 2.0:\n",
    "        return 1\n",
    "    elif value >= 2.5 and value <= 3.0:\n",
    "        return 1\n",
    "    elif value >= 3.5 and value <= 4.0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_locID</th>\n",
       "      <th>Hotel_geoID_x</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Date_of_stay</th>\n",
       "      <th>Trip_type</th>\n",
       "      <th>Review_helpful_votes</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Contributions</th>\n",
       "      <th>Reviewer_helpful_vote</th>\n",
       "      <th>...</th>\n",
       "      <th>Hotel_rating</th>\n",
       "      <th>Location_score</th>\n",
       "      <th>Resaurant_count</th>\n",
       "      <th>Attractions_count</th>\n",
       "      <th>Hotel_styles</th>\n",
       "      <th>Popular_mentions</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Review</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Unreliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>913352673</td>\n",
       "      <td>23-Aug</td>\n",
       "      <td>Traveled solo</td>\n",
       "      <td>0</td>\n",
       "      <td>Decent for the Price, but Lacks Attention to D...</td>\n",
       "      <td>23-Aug</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>508</td>\n",
       "      <td>[recently, got, stay, spot, x, hotel, part, re...</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>833765796</td>\n",
       "      <td>22-Feb</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>2</td>\n",
       "      <td>Best Choice ???ÂÂ» Stay in Orlando</td>\n",
       "      <td>22-Apr</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>45</td>\n",
       "      <td>[perfect, stay, smart, coice, brand, new, exce...</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>906461005</td>\n",
       "      <td>23-Jul</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>Gem of a hotel offering great value!</td>\n",
       "      <td>23-Jul</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>143</td>\n",
       "      <td>[restful, stay, july, wonderful, experience, s...</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>898109791</td>\n",
       "      <td>23-Jun</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Stay!!</td>\n",
       "      <td>23-Jun</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>47</td>\n",
       "      <td>[everything, hotel, great, !, excellent, rooms...</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23144733</td>\n",
       "      <td>34515</td>\n",
       "      <td>891583554</td>\n",
       "      <td>22-Nov</td>\n",
       "      <td>Traveled on business</td>\n",
       "      <td>0</td>\n",
       "      <td>New for now</td>\n",
       "      <td>23-May</td>\n",
       "      <td>149</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outlet mall', 'excellent cost benefit', 'exc...</td>\n",
       "      <td>63</td>\n",
       "      <td>[stay, week, beds, comfortable, clean, room, l...</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32287</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>417505916</td>\n",
       "      <td>16-Aug</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Worse hotel ever</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>209</td>\n",
       "      <td>[stayed, hotel, low, price, take, long, realiz...</td>\n",
       "      <td>-0.9830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32288</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>415134854</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>Traveled as a couple</td>\n",
       "      <td>0</td>\n",
       "      <td>WORST HOTEL I have EVER stayed at</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>130</td>\n",
       "      <td>[promise, never, ever, use, hotel, first, take...</td>\n",
       "      <td>0.6428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32289</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>413963936</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Will NEVER STAY here again</td>\n",
       "      <td>16-Sep</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>49</td>\n",
       "      <td>[first, roaches, room, live, dead, mold, bathr...</td>\n",
       "      <td>-0.9001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32290</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>395650617</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Not horrible but far from doable for more than...</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>145</td>\n",
       "      <td>[walking, took, forever, get, checked, sure, k...</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32291</th>\n",
       "      <td>2290607</td>\n",
       "      <td>34515</td>\n",
       "      <td>394406121</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>Traveled with family</td>\n",
       "      <td>0</td>\n",
       "      <td>Overnight stay when moving daughter.</td>\n",
       "      <td>16-Jul</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Budget']</td>\n",
       "      <td>[]</td>\n",
       "      <td>102</td>\n",
       "      <td>[although, price, reasonable, including, tax, ...</td>\n",
       "      <td>0.6093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30646 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hotel_locID  Hotel_geoID_x  Review_id Date_of_stay  \\\n",
       "0         23144733          34515  913352673       23-Aug   \n",
       "1         23144733          34515  833765796       22-Feb   \n",
       "2         23144733          34515  906461005       23-Jul   \n",
       "3         23144733          34515  898109791       23-Jun   \n",
       "4         23144733          34515  891583554       22-Nov   \n",
       "...            ...            ...        ...          ...   \n",
       "32287      2290607          34515  417505916       16-Aug   \n",
       "32288      2290607          34515  415134854       16-Sep   \n",
       "32289      2290607          34515  413963936       16-Sep   \n",
       "32290      2290607          34515  395650617       16-Jul   \n",
       "32291      2290607          34515  394406121       16-Jul   \n",
       "\n",
       "                  Trip_type  Review_helpful_votes  \\\n",
       "0             Traveled solo                     0   \n",
       "1      Traveled on business                     2   \n",
       "2      Traveled as a couple                     0   \n",
       "3      Traveled on business                     1   \n",
       "4      Traveled on business                     0   \n",
       "...                     ...                   ...   \n",
       "32287  Traveled with family                     0   \n",
       "32288  Traveled as a couple                     0   \n",
       "32289  Traveled with family                     0   \n",
       "32290                   NaN                     0   \n",
       "32291  Traveled with family                     0   \n",
       "\n",
       "                                                   Title Review_Date  \\\n",
       "0      Decent for the Price, but Lacks Attention to D...      23-Aug   \n",
       "1                    Best Choice ???ÂÂ» Stay in Orlando      22-Apr   \n",
       "2                   Gem of a hotel offering great value!      23-Jul   \n",
       "3                                       Excellent Stay!!      23-Jun   \n",
       "4                                            New for now      23-May   \n",
       "...                                                  ...         ...   \n",
       "32287                                   Worse hotel ever      16-Sep   \n",
       "32288                  WORST HOTEL I have EVER stayed at      16-Sep   \n",
       "32289                         Will NEVER STAY here again      16-Sep   \n",
       "32290  Not horrible but far from doable for more than...      16-Jul   \n",
       "32291               Overnight stay when moving daughter.      16-Jul   \n",
       "\n",
       "       Reviewer_Contributions  Reviewer_helpful_vote  ...  Hotel_rating  \\\n",
       "0                           8                      0  ...           4.5   \n",
       "1                           2                      4  ...           4.5   \n",
       "2                          19                     29  ...           4.5   \n",
       "3                           0                      0  ...           4.5   \n",
       "4                         149                     62  ...           4.5   \n",
       "...                       ...                    ...  ...           ...   \n",
       "32287                       0                      2  ...           1.5   \n",
       "32288                       0                      2  ...           1.5   \n",
       "32289                      13                      2  ...           1.5   \n",
       "32290                       0                      0  ...           1.5   \n",
       "32291                      53                     19  ...           1.5   \n",
       "\n",
       "       Location_score  Resaurant_count  Attractions_count  Hotel_styles  \\\n",
       "0                67.0             52.0                4.0            []   \n",
       "1                67.0             52.0                4.0            []   \n",
       "2                67.0             52.0                4.0            []   \n",
       "3                67.0             52.0                4.0            []   \n",
       "4                67.0             52.0                4.0            []   \n",
       "...               ...              ...                ...           ...   \n",
       "32287             NaN              NaN                NaN    ['Budget']   \n",
       "32288             NaN              NaN                NaN    ['Budget']   \n",
       "32289             NaN              NaN                NaN    ['Budget']   \n",
       "32290             NaN              NaN                NaN    ['Budget']   \n",
       "32291             NaN              NaN                NaN    ['Budget']   \n",
       "\n",
       "                                        Popular_mentions  WordCount  \\\n",
       "0      ['outlet mall', 'excellent cost benefit', 'exc...        508   \n",
       "1      ['outlet mall', 'excellent cost benefit', 'exc...         45   \n",
       "2      ['outlet mall', 'excellent cost benefit', 'exc...        143   \n",
       "3      ['outlet mall', 'excellent cost benefit', 'exc...         47   \n",
       "4      ['outlet mall', 'excellent cost benefit', 'exc...         63   \n",
       "...                                                  ...        ...   \n",
       "32287                                                 []        209   \n",
       "32288                                                 []        130   \n",
       "32289                                                 []         49   \n",
       "32290                                                 []        145   \n",
       "32291                                                 []        102   \n",
       "\n",
       "                                                  Review Compound_Score  \\\n",
       "0      [recently, got, stay, spot, x, hotel, part, re...         0.9967   \n",
       "1      [perfect, stay, smart, coice, brand, new, exce...         0.9584   \n",
       "2      [restful, stay, july, wonderful, experience, s...         0.9902   \n",
       "3      [everything, hotel, great, !, excellent, rooms...         0.9390   \n",
       "4      [stay, week, beds, comfortable, clean, room, l...         0.2363   \n",
       "...                                                  ...            ...   \n",
       "32287  [stayed, hotel, low, price, take, long, realiz...        -0.9830   \n",
       "32288  [promise, never, ever, use, hotel, first, take...         0.6428   \n",
       "32289  [first, roaches, room, live, dead, mold, bathr...        -0.9001   \n",
       "32290  [walking, took, forever, get, checked, sure, k...         0.8705   \n",
       "32291  [although, price, reasonable, including, tax, ...         0.6093   \n",
       "\n",
       "       Unreliable  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "32287           0  \n",
       "32288           0  \n",
       "32289           0  \n",
       "32290           0  \n",
       "32291           0  \n",
       "\n",
       "[30646 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new columns\n",
    "# text_preprocessed_df['Reviewer_Contributions_range'] = text_preprocessed_df['Reviewer_Contributions'].apply(contributions_range)\n",
    "# text_preprocessed_df['Hotel_star_range'] = text_preprocessed_df['Hotel_star'].apply(hotel_star_range)\n",
    "text_preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 4 sub-datasets\n",
    "# LCLS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 1) & (text_preprocessed_df['Hotel_star_range'] == 1)]\n",
    "# LCHS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 1) & (text_preprocessed_df['Hotel_star_range'] == 2)]\n",
    "# HCLS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 2) & (text_preprocessed_df['Hotel_star_range'] == 1)]\n",
    "# HCHS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 2) & (text_preprocessed_df['Hotel_star_range'] == 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split unreliable reviews\n",
    "Outdoor_unreliable = text_preprocessed_df[text_preprocessed_df['Unreliable'] == 1]\n",
    "# LCLS_unreliable = LCLS[LCLS['Unreliable'] == 1]\n",
    "# LCHS_unreliable = LCHS[LCHS['Unreliable'] == 1]\n",
    "# HCLS_unreliable = HCLS[HCLS['Unreliable'] == 1]\n",
    "# HCHS_unreliable = HCHS[HCHS['Unreliable'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split reliable reviews\n",
    "Outdoor_reliable = text_preprocessed_df[text_preprocessed_df['Unreliable'] == 0]\n",
    "# LCLS_reliable = LCLS[LCLS['Unreliable'] == 0]\n",
    "# LCHS_reliable = LCHS[LCHS['Unreliable'] == 0]\n",
    "# HCLS_reliable = HCLS[HCLS['Unreliable'] == 0]\n",
    "# HCHS_reliable = HCHS[HCHS['Unreliable'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split subset\n",
    "selected_columns = ['Review', 'Review_Rating']\n",
    "Outdoor_reliable_text = Outdoor_reliable.loc[:, selected_columns]\n",
    "\n",
    "Outdoor_reliable_text.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Outdoor_reliable_text['Review'] = [' '.join(text) for text in Outdoor_reliable_text['Review']]\n",
    "# LCLS_text = LCLS_reliable.loc[:, selected_columns]\n",
    "# LCHS_text = LCHS_reliable.loc[:, selected_columns]\n",
    "# HCLS_text = HCLS_reliable.loc[:, selected_columns]\n",
    "# HCHS_text = HCHS_reliable.loc[:, selected_columns]\n",
    "\n",
    "# # reset index\n",
    "# LCLS_text.reset_index(drop=True, inplace=True)\n",
    "# LCHS_text.reset_index(drop=True, inplace=True)\n",
    "# HCLS_text.reset_index(drop=True, inplace=True)\n",
    "# HCHS_text.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split sub-datasets to X & Y, Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS_X = LCLS_text['Review']\n",
    "# LCLS_y = LCLS_text['Review_Rating']\n",
    "\n",
    "# LCHS_X = LCHS_text['Review']\n",
    "# LCHS_y = LCHS_text['Review_Rating']\n",
    "\n",
    "# HCLS_X = HCLS_text['Review']\n",
    "# HCLS_y = HCLS_text['Review_Rating']\n",
    "\n",
    "# HCHS_X = HCHS_text['Review']\n",
    "# HCHS_y = HCHS_text['Review_Rating']\n",
    "\n",
    "Outdoor_X = Outdoor_reliable_text['Review']\n",
    "Outdoor_y = Outdoor_reliable_text['Review_Rating']\n",
    "\n",
    "Outdoor_X_train, Outdoor_X_test, Outdoor_y_train, Outdoor_y_test = train_test_split(Outdoor_X, Outdoor_y, test_size=0.2, random_state=88)\n",
    "# LCLS_X_train, LCLS_X_test, LCLS_y_train, LCLS_y_test = train_test_split(LCLS_X, LCLS_y, test_size=0.2, random_state=88)\n",
    "# LCHS_X_train, LCHS_X_test, LCHS_y_train, LCHS_y_test = train_test_split(LCHS_X, LCHS_y, test_size=0.2, random_state=88)\n",
    "# HCLS_X_train, HCLS_X_test, HCLS_y_train, HCLS_y_test = train_test_split(HCLS_X, HCLS_y, test_size=0.2, random_state=88)\n",
    "# HCHS_X_train, HCHS_X_test, HCHS_y_train, HCHS_y_test = train_test_split(HCHS_X, HCHS_y, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8997     absolute home home florida hotel even microwav...\n",
       "12421    hotel clean kids enjoyed pool gym bar amazing ...\n",
       "2511     one worst hotel stayed ever mean hear wi fi pa...\n",
       "21441    best place stay orlando florida place extra cl...\n",
       "6496     able find many reviews prior trip wanted share...\n",
       "                               ...                        \n",
       "24938    worst hotel ever took daughter due volleyball ...\n",
       "2481     service great ! grounds large front desk gentl...\n",
       "4047     first time staying came back service provided ...\n",
       "6432     family super hard day came sheraton covid pcr ...\n",
       "26584    stay worst hotel ever caries wyndham name dirt...\n",
       "Name: Review, Length: 24075, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outdoor_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8997     5\n",
       "12421    5\n",
       "2511     2\n",
       "21441    5\n",
       "6496     4\n",
       "        ..\n",
       "24938    1\n",
       "2481     4\n",
       "4047     5\n",
       "6432     5\n",
       "26584    1\n",
       "Name: Review_Rating, Length: 24075, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outdoor_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29093    stayed hotel year old son mixed business mini ...\n",
       "14732    rooms great spacious pool area always clean fu...\n",
       "27345    held business training one meeting spaces cour...\n",
       "28049    first say booked dvc property cash stay joined...\n",
       "15621    great staff room needs refreshing drapes close...\n",
       "                               ...                        \n",
       "16494    night stay check process took long time booked...\n",
       "15829    breakfast vending machines laundry machines ic...\n",
       "19355    worked golf tournament bay hill days long tiri...\n",
       "18896    room completely clean found cockroaches dusty ...\n",
       "17314    would given stars could attraction park sad fr...\n",
       "Name: Review, Length: 6019, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outdoor_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29093    5\n",
       "14732    5\n",
       "27345    5\n",
       "28049    5\n",
       "15621    4\n",
       "        ..\n",
       "16494    3\n",
       "15829    1\n",
       "19355    5\n",
       "18896    1\n",
       "17314    1\n",
       "Name: Review_Rating, Length: 6019, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outdoor_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def generate_bow_train(X_train, max_features=1000):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    bow_vectors = vectorizer.fit_transform(X_train)\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bow_df, vectorizer\n",
    "\n",
    "def generate_bow_test(X_test, vectorizer):\n",
    "    bow_vectors = vectorizer.transform(X_test)\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_train_bow_df, Outdoor_vectorizer = generate_bow_train(Outdoor_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_test_bow_df = generate_bow_test(Outdoor_X_test, Outdoor_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def generate_tfidf_train(X_train, stop_words='english', max_features=1000, max_df=0.9):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features, max_df=max_df)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(X_train_tfidf, columns=feature_names)\n",
    "    \n",
    "    return tfidf_df, tfidf_vectorizer\n",
    "\n",
    "def generate_tfidf_test(X_test, tfidf_vectorizer):\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(X_test_tfidf, columns=feature_names)\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_train_tfidf_df, tfidf_vectorizer_Outdoor = generate_tfidf_train(Outdoor_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_test_tfidf_df = generate_tfidf_test(Outdoor_X_test, tfidf_vectorizer_Outdoor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def train_doc2vec_model(X_train, min_count=5, workers=8, epochs=40, vector_size=100):\n",
    "    tagged_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(X_train)]\n",
    "    model = Doc2Vec(min_count=min_count, workers=workers, epochs=epochs, vector_size=vector_size)\n",
    "    model.build_vocab(tagged_docs)\n",
    "    model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2V_model = train_doc2vec_model(Outdoor_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_train_d2v = pd.DataFrame(np.array([D2V_model.infer_vector((doc.split(' '))) for doc in Outdoor_X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.774833</td>\n",
       "      <td>-0.143001</td>\n",
       "      <td>-0.892874</td>\n",
       "      <td>-0.233048</td>\n",
       "      <td>0.653453</td>\n",
       "      <td>-0.668397</td>\n",
       "      <td>0.012768</td>\n",
       "      <td>-0.169828</td>\n",
       "      <td>0.072204</td>\n",
       "      <td>-0.359969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318764</td>\n",
       "      <td>-0.079619</td>\n",
       "      <td>-0.119047</td>\n",
       "      <td>0.314340</td>\n",
       "      <td>0.372970</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>-0.506409</td>\n",
       "      <td>0.165405</td>\n",
       "      <td>-0.929570</td>\n",
       "      <td>-0.180259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.169286</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>-0.286137</td>\n",
       "      <td>-0.053463</td>\n",
       "      <td>-0.377443</td>\n",
       "      <td>-0.923604</td>\n",
       "      <td>-0.005215</td>\n",
       "      <td>1.229463</td>\n",
       "      <td>-0.736206</td>\n",
       "      <td>-0.551008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085528</td>\n",
       "      <td>0.227806</td>\n",
       "      <td>-0.610951</td>\n",
       "      <td>-0.139019</td>\n",
       "      <td>-1.029508</td>\n",
       "      <td>1.005347</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.283024</td>\n",
       "      <td>-0.038161</td>\n",
       "      <td>0.092901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.106345</td>\n",
       "      <td>-0.213564</td>\n",
       "      <td>-1.268841</td>\n",
       "      <td>-0.152352</td>\n",
       "      <td>0.521268</td>\n",
       "      <td>-0.502670</td>\n",
       "      <td>0.730943</td>\n",
       "      <td>-1.525511</td>\n",
       "      <td>-2.022004</td>\n",
       "      <td>0.142890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310705</td>\n",
       "      <td>-0.586734</td>\n",
       "      <td>0.685515</td>\n",
       "      <td>1.430984</td>\n",
       "      <td>0.140491</td>\n",
       "      <td>-0.024617</td>\n",
       "      <td>-0.817919</td>\n",
       "      <td>-0.408232</td>\n",
       "      <td>-0.738587</td>\n",
       "      <td>-0.157084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.797326</td>\n",
       "      <td>0.515016</td>\n",
       "      <td>-0.727590</td>\n",
       "      <td>0.918477</td>\n",
       "      <td>0.348959</td>\n",
       "      <td>-0.736819</td>\n",
       "      <td>-0.551690</td>\n",
       "      <td>-0.149062</td>\n",
       "      <td>-1.718782</td>\n",
       "      <td>-1.267354</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.057470</td>\n",
       "      <td>0.197350</td>\n",
       "      <td>0.379467</td>\n",
       "      <td>-0.219875</td>\n",
       "      <td>0.399387</td>\n",
       "      <td>-0.329094</td>\n",
       "      <td>0.139156</td>\n",
       "      <td>-0.335063</td>\n",
       "      <td>-0.282928</td>\n",
       "      <td>-0.622677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.926168</td>\n",
       "      <td>-0.428331</td>\n",
       "      <td>-0.097422</td>\n",
       "      <td>1.633352</td>\n",
       "      <td>-0.644155</td>\n",
       "      <td>0.447637</td>\n",
       "      <td>-0.453105</td>\n",
       "      <td>-1.826963</td>\n",
       "      <td>-0.652638</td>\n",
       "      <td>-1.184929</td>\n",
       "      <td>...</td>\n",
       "      <td>1.730912</td>\n",
       "      <td>1.649688</td>\n",
       "      <td>1.788185</td>\n",
       "      <td>0.885802</td>\n",
       "      <td>1.079745</td>\n",
       "      <td>0.866907</td>\n",
       "      <td>2.121641</td>\n",
       "      <td>-0.460191</td>\n",
       "      <td>-0.730750</td>\n",
       "      <td>1.892881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24070</th>\n",
       "      <td>0.481946</td>\n",
       "      <td>0.454467</td>\n",
       "      <td>-0.259467</td>\n",
       "      <td>0.942835</td>\n",
       "      <td>0.053195</td>\n",
       "      <td>-0.108638</td>\n",
       "      <td>0.494477</td>\n",
       "      <td>0.153143</td>\n",
       "      <td>-0.666396</td>\n",
       "      <td>-0.230788</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.086304</td>\n",
       "      <td>-0.278006</td>\n",
       "      <td>0.295645</td>\n",
       "      <td>-0.465999</td>\n",
       "      <td>0.677841</td>\n",
       "      <td>-0.598442</td>\n",
       "      <td>-0.628338</td>\n",
       "      <td>-1.091451</td>\n",
       "      <td>0.715241</td>\n",
       "      <td>-0.426479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24071</th>\n",
       "      <td>-0.636638</td>\n",
       "      <td>-0.300922</td>\n",
       "      <td>0.111902</td>\n",
       "      <td>0.170197</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>0.100870</td>\n",
       "      <td>0.803125</td>\n",
       "      <td>0.623899</td>\n",
       "      <td>-0.639687</td>\n",
       "      <td>-0.587034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204110</td>\n",
       "      <td>0.755421</td>\n",
       "      <td>0.111540</td>\n",
       "      <td>-0.645747</td>\n",
       "      <td>0.140160</td>\n",
       "      <td>0.309067</td>\n",
       "      <td>0.438182</td>\n",
       "      <td>-0.315385</td>\n",
       "      <td>0.508518</td>\n",
       "      <td>-0.041458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24072</th>\n",
       "      <td>-1.171554</td>\n",
       "      <td>-0.921848</td>\n",
       "      <td>-0.820316</td>\n",
       "      <td>0.115672</td>\n",
       "      <td>0.794387</td>\n",
       "      <td>-1.060208</td>\n",
       "      <td>-0.198911</td>\n",
       "      <td>0.651164</td>\n",
       "      <td>-0.500871</td>\n",
       "      <td>-0.934945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725993</td>\n",
       "      <td>0.040019</td>\n",
       "      <td>-0.015844</td>\n",
       "      <td>0.535494</td>\n",
       "      <td>-0.463717</td>\n",
       "      <td>-0.714822</td>\n",
       "      <td>-0.310968</td>\n",
       "      <td>0.938007</td>\n",
       "      <td>0.636888</td>\n",
       "      <td>-0.267429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24073</th>\n",
       "      <td>-0.202422</td>\n",
       "      <td>0.289605</td>\n",
       "      <td>-0.057572</td>\n",
       "      <td>-0.110857</td>\n",
       "      <td>-0.028276</td>\n",
       "      <td>-0.435836</td>\n",
       "      <td>-0.003955</td>\n",
       "      <td>1.208357</td>\n",
       "      <td>-0.950352</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583582</td>\n",
       "      <td>-0.226097</td>\n",
       "      <td>0.142387</td>\n",
       "      <td>-1.697186</td>\n",
       "      <td>-0.441318</td>\n",
       "      <td>-0.159669</td>\n",
       "      <td>-2.084000</td>\n",
       "      <td>-0.386158</td>\n",
       "      <td>-2.012048</td>\n",
       "      <td>-0.141233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24074</th>\n",
       "      <td>-0.708693</td>\n",
       "      <td>-1.007879</td>\n",
       "      <td>0.179741</td>\n",
       "      <td>-1.760467</td>\n",
       "      <td>-0.749291</td>\n",
       "      <td>-0.730976</td>\n",
       "      <td>0.869650</td>\n",
       "      <td>-0.130858</td>\n",
       "      <td>-1.571185</td>\n",
       "      <td>-0.740067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429254</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>-0.736965</td>\n",
       "      <td>-1.180935</td>\n",
       "      <td>-0.028008</td>\n",
       "      <td>-0.032744</td>\n",
       "      <td>-0.465980</td>\n",
       "      <td>0.540602</td>\n",
       "      <td>0.312976</td>\n",
       "      <td>0.437284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24075 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.774833 -0.143001 -0.892874 -0.233048  0.653453 -0.668397  0.012768   \n",
       "1     -1.169286  0.011649 -0.286137 -0.053463 -0.377443 -0.923604 -0.005215   \n",
       "2     -0.106345 -0.213564 -1.268841 -0.152352  0.521268 -0.502670  0.730943   \n",
       "3     -1.797326  0.515016 -0.727590  0.918477  0.348959 -0.736819 -0.551690   \n",
       "4     -0.926168 -0.428331 -0.097422  1.633352 -0.644155  0.447637 -0.453105   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24070  0.481946  0.454467 -0.259467  0.942835  0.053195 -0.108638  0.494477   \n",
       "24071 -0.636638 -0.300922  0.111902  0.170197 -0.055298  0.100870  0.803125   \n",
       "24072 -1.171554 -0.921848 -0.820316  0.115672  0.794387 -1.060208 -0.198911   \n",
       "24073 -0.202422  0.289605 -0.057572 -0.110857 -0.028276 -0.435836 -0.003955   \n",
       "24074 -0.708693 -1.007879  0.179741 -1.760467 -0.749291 -0.730976  0.869650   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0     -0.169828  0.072204 -0.359969  ...  0.318764 -0.079619 -0.119047   \n",
       "1      1.229463 -0.736206 -0.551008  ... -0.085528  0.227806 -0.610951   \n",
       "2     -1.525511 -2.022004  0.142890  ...  0.310705 -0.586734  0.685515   \n",
       "3     -0.149062 -1.718782 -1.267354  ... -1.057470  0.197350  0.379467   \n",
       "4     -1.826963 -0.652638 -1.184929  ...  1.730912  1.649688  1.788185   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24070  0.153143 -0.666396 -0.230788  ... -1.086304 -0.278006  0.295645   \n",
       "24071  0.623899 -0.639687 -0.587034  ...  0.204110  0.755421  0.111540   \n",
       "24072  0.651164 -0.500871 -0.934945  ...  0.725993  0.040019 -0.015844   \n",
       "24073  1.208357 -0.950352  0.024835  ...  0.583582 -0.226097  0.142387   \n",
       "24074 -0.130858 -1.571185 -0.740067  ...  0.429254  0.417800 -0.736965   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.314340  0.372970  0.008912 -0.506409  0.165405 -0.929570 -0.180259  \n",
       "1     -0.139019 -1.029508  1.005347  0.009634  0.283024 -0.038161  0.092901  \n",
       "2      1.430984  0.140491 -0.024617 -0.817919 -0.408232 -0.738587 -0.157084  \n",
       "3     -0.219875  0.399387 -0.329094  0.139156 -0.335063 -0.282928 -0.622677  \n",
       "4      0.885802  1.079745  0.866907  2.121641 -0.460191 -0.730750  1.892881  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24070 -0.465999  0.677841 -0.598442 -0.628338 -1.091451  0.715241 -0.426479  \n",
       "24071 -0.645747  0.140160  0.309067  0.438182 -0.315385  0.508518 -0.041458  \n",
       "24072  0.535494 -0.463717 -0.714822 -0.310968  0.938007  0.636888 -0.267429  \n",
       "24073 -1.697186 -0.441318 -0.159669 -2.084000 -0.386158 -2.012048 -0.141233  \n",
       "24074 -1.180935 -0.028008 -0.032744 -0.465980  0.540602  0.312976  0.437284  \n",
       "\n",
       "[24075 rows x 100 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outdoor_X_train_d2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_test_d2v = pd.DataFrame(np.array([D2V_model.infer_vector((doc.split(' '))) for doc in Outdoor_X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../../GloVe_wordvec/glove.6B.100d.txt'\n",
    "\n",
    "# import GloVe word vectors into dictionary\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create embedding matrix\n",
    "def glove_embedding(comment, embeddings_index = embeddings_index, dim=100):\n",
    "    words = comment.split()\n",
    "    vec = np.zeros(dim)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word) # get GloVe word vectors\n",
    "        if embedding_vector is not None:\n",
    "            vec += embedding_vector\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_train_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in Outdoor_X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.233635</td>\n",
       "      <td>0.430726</td>\n",
       "      <td>-0.356195</td>\n",
       "      <td>-0.240488</td>\n",
       "      <td>0.332360</td>\n",
       "      <td>0.049685</td>\n",
       "      <td>0.252141</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>-0.021626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170154</td>\n",
       "      <td>-0.097273</td>\n",
       "      <td>0.118134</td>\n",
       "      <td>0.099495</td>\n",
       "      <td>-0.573068</td>\n",
       "      <td>0.016141</td>\n",
       "      <td>-0.098072</td>\n",
       "      <td>-0.174082</td>\n",
       "      <td>0.488518</td>\n",
       "      <td>0.263079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060790</td>\n",
       "      <td>0.187020</td>\n",
       "      <td>0.243104</td>\n",
       "      <td>-0.165561</td>\n",
       "      <td>-0.124957</td>\n",
       "      <td>0.134420</td>\n",
       "      <td>-0.086875</td>\n",
       "      <td>0.561494</td>\n",
       "      <td>-0.233715</td>\n",
       "      <td>-0.018706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107866</td>\n",
       "      <td>-0.122068</td>\n",
       "      <td>0.253637</td>\n",
       "      <td>-0.071931</td>\n",
       "      <td>-0.596894</td>\n",
       "      <td>0.089571</td>\n",
       "      <td>-0.246285</td>\n",
       "      <td>-0.157705</td>\n",
       "      <td>0.452043</td>\n",
       "      <td>0.179871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034770</td>\n",
       "      <td>0.248121</td>\n",
       "      <td>0.330794</td>\n",
       "      <td>-0.204502</td>\n",
       "      <td>-0.127864</td>\n",
       "      <td>0.306917</td>\n",
       "      <td>-0.136960</td>\n",
       "      <td>0.292214</td>\n",
       "      <td>-0.007233</td>\n",
       "      <td>-0.123031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>-0.099344</td>\n",
       "      <td>0.225505</td>\n",
       "      <td>-0.140386</td>\n",
       "      <td>-0.411669</td>\n",
       "      <td>0.076435</td>\n",
       "      <td>-0.094175</td>\n",
       "      <td>-0.079918</td>\n",
       "      <td>0.423178</td>\n",
       "      <td>0.131202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010523</td>\n",
       "      <td>0.202525</td>\n",
       "      <td>0.343270</td>\n",
       "      <td>-0.080774</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>0.061599</td>\n",
       "      <td>-0.003004</td>\n",
       "      <td>0.153365</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>-0.122573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014472</td>\n",
       "      <td>-0.054335</td>\n",
       "      <td>0.098424</td>\n",
       "      <td>-0.185599</td>\n",
       "      <td>-0.390590</td>\n",
       "      <td>0.091024</td>\n",
       "      <td>-0.096778</td>\n",
       "      <td>-0.085739</td>\n",
       "      <td>0.267129</td>\n",
       "      <td>0.190479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.082540</td>\n",
       "      <td>0.202572</td>\n",
       "      <td>0.157080</td>\n",
       "      <td>0.075078</td>\n",
       "      <td>-0.045980</td>\n",
       "      <td>0.370636</td>\n",
       "      <td>-0.052210</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>-0.075156</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060270</td>\n",
       "      <td>-0.238311</td>\n",
       "      <td>0.279981</td>\n",
       "      <td>-0.149263</td>\n",
       "      <td>-0.369926</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>-0.082832</td>\n",
       "      <td>-0.129802</td>\n",
       "      <td>0.610468</td>\n",
       "      <td>0.066063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24070</th>\n",
       "      <td>0.097162</td>\n",
       "      <td>0.136113</td>\n",
       "      <td>0.303871</td>\n",
       "      <td>-0.158388</td>\n",
       "      <td>-0.129692</td>\n",
       "      <td>0.250260</td>\n",
       "      <td>-0.076399</td>\n",
       "      <td>0.311117</td>\n",
       "      <td>-0.033105</td>\n",
       "      <td>-0.121727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>-0.126546</td>\n",
       "      <td>0.029301</td>\n",
       "      <td>-0.012669</td>\n",
       "      <td>-0.421316</td>\n",
       "      <td>0.067971</td>\n",
       "      <td>-0.143759</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.398897</td>\n",
       "      <td>0.100250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24071</th>\n",
       "      <td>-0.109093</td>\n",
       "      <td>0.192889</td>\n",
       "      <td>0.235313</td>\n",
       "      <td>-0.333612</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>0.246799</td>\n",
       "      <td>0.040152</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>-0.015179</td>\n",
       "      <td>-0.112485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146336</td>\n",
       "      <td>-0.086903</td>\n",
       "      <td>0.139536</td>\n",
       "      <td>0.084781</td>\n",
       "      <td>-0.483748</td>\n",
       "      <td>-0.071064</td>\n",
       "      <td>-0.202578</td>\n",
       "      <td>-0.131936</td>\n",
       "      <td>0.422437</td>\n",
       "      <td>0.399555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24072</th>\n",
       "      <td>-0.021392</td>\n",
       "      <td>0.180661</td>\n",
       "      <td>0.270321</td>\n",
       "      <td>-0.075594</td>\n",
       "      <td>-0.084365</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>-0.175848</td>\n",
       "      <td>0.178269</td>\n",
       "      <td>-0.011167</td>\n",
       "      <td>-0.103454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042484</td>\n",
       "      <td>-0.048000</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>-0.151392</td>\n",
       "      <td>-0.445246</td>\n",
       "      <td>-0.012530</td>\n",
       "      <td>0.067934</td>\n",
       "      <td>-0.024743</td>\n",
       "      <td>0.303343</td>\n",
       "      <td>0.167260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24073</th>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.134537</td>\n",
       "      <td>0.405518</td>\n",
       "      <td>-0.243083</td>\n",
       "      <td>-0.252662</td>\n",
       "      <td>0.173529</td>\n",
       "      <td>-0.036085</td>\n",
       "      <td>0.377412</td>\n",
       "      <td>-0.250863</td>\n",
       "      <td>-0.107711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052560</td>\n",
       "      <td>-0.039903</td>\n",
       "      <td>0.148470</td>\n",
       "      <td>-0.100287</td>\n",
       "      <td>-0.503458</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>-0.037735</td>\n",
       "      <td>-0.056240</td>\n",
       "      <td>0.369959</td>\n",
       "      <td>0.183662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24074</th>\n",
       "      <td>-0.034701</td>\n",
       "      <td>0.187308</td>\n",
       "      <td>0.216356</td>\n",
       "      <td>-0.163629</td>\n",
       "      <td>-0.140735</td>\n",
       "      <td>0.252806</td>\n",
       "      <td>-0.039976</td>\n",
       "      <td>0.383469</td>\n",
       "      <td>-0.049464</td>\n",
       "      <td>-0.002194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144819</td>\n",
       "      <td>-0.258429</td>\n",
       "      <td>0.111015</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>-0.362502</td>\n",
       "      <td>0.019933</td>\n",
       "      <td>-0.006333</td>\n",
       "      <td>-0.011914</td>\n",
       "      <td>0.413762</td>\n",
       "      <td>0.060789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24075 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.001987  0.233635  0.430726 -0.356195 -0.240488  0.332360  0.049685   \n",
       "1      0.060790  0.187020  0.243104 -0.165561 -0.124957  0.134420 -0.086875   \n",
       "2     -0.034770  0.248121  0.330794 -0.204502 -0.127864  0.306917 -0.136960   \n",
       "3     -0.010523  0.202525  0.343270 -0.080774  0.011234  0.061599 -0.003004   \n",
       "4     -0.082540  0.202572  0.157080  0.075078 -0.045980  0.370636 -0.052210   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24070  0.097162  0.136113  0.303871 -0.158388 -0.129692  0.250260 -0.076399   \n",
       "24071 -0.109093  0.192889  0.235313 -0.333612 -0.093659  0.246799  0.040152   \n",
       "24072 -0.021392  0.180661  0.270321 -0.075594 -0.084365  0.065231 -0.175848   \n",
       "24073  0.008169  0.134537  0.405518 -0.243083 -0.252662  0.173529 -0.036085   \n",
       "24074 -0.034701  0.187308  0.216356 -0.163629 -0.140735  0.252806 -0.039976   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.252141  0.014591 -0.021626  ...  0.170154 -0.097273  0.118134   \n",
       "1      0.561494 -0.233715 -0.018706  ...  0.107866 -0.122068  0.253637   \n",
       "2      0.292214 -0.007233 -0.123031  ...  0.092045 -0.099344  0.225505   \n",
       "3      0.153365 -0.152003 -0.122573  ... -0.014472 -0.054335  0.098424   \n",
       "4      0.400990 -0.075156  0.022261  ...  0.060270 -0.238311  0.279981   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24070  0.311117 -0.033105 -0.121727  ...  0.030026 -0.126546  0.029301   \n",
       "24071  0.143069 -0.015179 -0.112485  ...  0.146336 -0.086903  0.139536   \n",
       "24072  0.178269 -0.011167 -0.103454  ... -0.042484 -0.048000 -0.014793   \n",
       "24073  0.377412 -0.250863 -0.107711  ...  0.052560 -0.039903  0.148470   \n",
       "24074  0.383469 -0.049464 -0.002194  ...  0.144819 -0.258429  0.111015   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.099495 -0.573068  0.016141 -0.098072 -0.174082  0.488518  0.263079  \n",
       "1     -0.071931 -0.596894  0.089571 -0.246285 -0.157705  0.452043  0.179871  \n",
       "2     -0.140386 -0.411669  0.076435 -0.094175 -0.079918  0.423178  0.131202  \n",
       "3     -0.185599 -0.390590  0.091024 -0.096778 -0.085739  0.267129  0.190479  \n",
       "4     -0.149263 -0.369926  0.056957 -0.082832 -0.129802  0.610468  0.066063  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24070 -0.012669 -0.421316  0.067971 -0.143759  0.013924  0.398897  0.100250  \n",
       "24071  0.084781 -0.483748 -0.071064 -0.202578 -0.131936  0.422437  0.399555  \n",
       "24072 -0.151392 -0.445246 -0.012530  0.067934 -0.024743  0.303343  0.167260  \n",
       "24073 -0.100287 -0.503458  0.025204 -0.037735 -0.056240  0.369959  0.183662  \n",
       "24074 -0.117024 -0.362502  0.019933 -0.006333 -0.011914  0.413762  0.060789  \n",
       "\n",
       "[24075 rows x 100 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outdoor_X_train_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "Outdoor_X_test_glove = pd.DataFrame(np.array([glove_embedding(comment) for comment in Outdoor_X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT(longformer) model\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = transformers.LongformerTokenizer.from_pretrained(model_name)\n",
    "model = transformers.LongformerModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def bert_embedding(X_train):\n",
    "    embeddings = []\n",
    "    for text in X_train:\n",
    "        # 將文本轉成BERT的輸入格式，即加上[CLS]與[SEP] token，並轉成tensor\n",
    "        encoded_text = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # 用預訓練BERT模型轉成向量\n",
    "        with torch.no_grad():\n",
    "            model_output = model(encoded_text['input_ids'], attention_mask=encoded_text['attention_mask'])\n",
    "\n",
    "        # 取出[CLS] token對應的向量作為整個文本的向量表示\n",
    "        embeddings.append(model_output.last_hidden_state[:, 0, :].squeeze().tolist())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_train_bert = pd.DataFrame(bert_embedding(Outdoor_X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outdoor_X_test_bert = pd.DataFrame(bert_embedding(Outdoor_X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package loading & function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 11:38:01.548215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-22 11:38:01.548239: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-22 11:38:01.548257: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-22 11:38:01.552056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 11:38:02.403678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# ML\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# DL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation function\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def validation(model_name, X_train, y_train, word_vec_train):\n",
    "    cv = 10\n",
    "    \n",
    "    # MSE\n",
    "    mse_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='neg_mean_squared_error') # mse\n",
    "    mse_scores = -mse_scores # transfer to positive\n",
    "    avg_mse = mse_scores.mean()\n",
    "    \n",
    "    # MAE\n",
    "    mae_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    mae_scores = -mae_scores  # Convert to positive values\n",
    "    avg_mae = mae_scores.mean()\n",
    "    \n",
    "    # MAPE\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)  # Create custom scorer\n",
    "    mape_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring=mape_scorer)\n",
    "    mape_scores = -mape_scores  # Convert to positive values\n",
    "    avg_mape = mape_scores.mean()\n",
    "    \n",
    "    # R-squared\n",
    "    r2_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='r2')\n",
    "    avg_r2 = r2_scores.mean()\n",
    "\n",
    "    print(f\"{word_vec_train}(val)'s MSE, MAE, MAPE, R^2: {avg_mse}, {avg_mae}, {avg_mape}, {avg_r2}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation\n",
    "def evaluation(y_test, y_pred, word_vec_test):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{word_vec_test}'s MSE, MAE, MAPE, R^2: {mse}, {mae}, {mape}, {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define EarlyStopping callback \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # 監控驗證集上的損失值\n",
    "    patience=10,  # 如果性能在10個epoch內沒有改善，則停止訓練\n",
    "    verbose=1,  \n",
    "    restore_best_weights=True  # 恢復最佳權重\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize word vector\n",
    "Data = {\n",
    "    \"tf\": {\n",
    "        'X_train': Outdoor_X_train_bow_df,\n",
    "        'X_test': Outdoor_X_test_bow_df\n",
    "    },\n",
    "    \"tf-idf\": {\n",
    "        'X_train': Outdoor_X_train_tfidf_df,\n",
    "        'X_test': Outdoor_X_test_tfidf_df\n",
    "    },\n",
    "    \"d2v\": {\n",
    "        'X_train': Outdoor_X_train_d2v,\n",
    "        'X_test': Outdoor_X_test_d2v\n",
    "    },\n",
    "    \"glove\": {\n",
    "        'X_train': Outdoor_X_train_glove,\n",
    "        'X_test': Outdoor_X_test_glove\n",
    "    },\n",
    "    \"bert\": {\n",
    "        'X_train': Outdoor_X_train_bert,\n",
    "        'X_test': Outdoor_X_test_bert\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML model\n",
    "* SVR\n",
    "* Random Forest\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model = SVR(epsilon=0.2, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's MSE, MAE, MAPE, R^2: 0.6597457450456092, 0.6085124098939498, 0.28882263130816765, 0.7091446879491003\n",
      "tf's MSE, MAE, MAPE, R^2: 0.6538957152771586, 0.5982381036975569, 0.2817317088022648, 0.7057595997963044\n",
      "X_train's MSE, MAE, MAPE, R^2: 0.5729556658144563, 0.572623262682101, 0.25983936445860156, 0.7474108526376219\n",
      "tf-idf's MSE, MAE, MAPE, R^2: 0.5628926065697711, 0.5607802150658922, 0.25239763974917834, 0.7467092351896065\n",
      "X_train's MSE, MAE, MAPE, R^2: 0.8063064394263249, 0.6699452013959886, 0.3244832634381806, 0.6445306132139924\n",
      "d2v's MSE, MAE, MAPE, R^2: 0.7806174240559924, 0.6688758331738645, 0.30928380396218436, 0.6487372865520955\n",
      "X_train's MSE, MAE, MAPE, R^2: 0.71458095650901, 0.629855781167544, 0.294591786167283, 0.684919231546911\n",
      "glove's MSE, MAE, MAPE, R^2: 0.7194881031344965, 0.6318938429333663, 0.293861147741169, 0.6762442963579287\n",
      "X_train's MSE, MAE, MAPE, R^2: 0.6422419934001791, 0.6129300403561981, 0.2863774705110866, 0.7168542170291279\n",
      "bert's MSE, MAE, MAPE, R^2: 0.6446319847773747, 0.6138904864097064, 0.2812072855334133, 0.7099280990018395\n"
     ]
    }
   ],
   "source": [
    "for word_vec in Data.keys():\n",
    "    word_vec_train = 'X_train'\n",
    "    word_vec_test = 'X_test'\n",
    "    \n",
    "    # print(Data[word_vec][word_vec_train])\n",
    "    \n",
    "    # train model\n",
    "    svr_model.fit(Data[word_vec][word_vec_train], Outdoor_y_train)\n",
    "    \n",
    "    validation(svr_model, Data[word_vec][word_vec_train], Outdoor_y_train, word_vec) # print validation result\n",
    "    \n",
    "    # predict\n",
    "    svr_y_pred_Outdoor = svr_model.predict(Data[word_vec][word_vec_test])\n",
    "    \n",
    "    # evaluate\n",
    "    # print(f\"Evaluating {word_vec}...\")\n",
    "    evaluation(Outdoor_y_test, svr_y_pred_Outdoor, word_vec)\n",
    "    print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf(val)'s MSE, MAE, MAPE, R^2: 0.7516157858937397, 0.6038884750500134, 0.27455763539905, 0.6686366377962237\n",
      "tf's MSE, MAE, MAPE, R^2: 0.7708792723597498, 0.610816636207565, 0.2765397795868638, 0.6531192661023548\n",
      "-------------------------------------------------\n",
      "tf-idf(val)'s MSE, MAE, MAPE, R^2: 0.7242295805425764, 0.6058049188505654, 0.2741163450974093, 0.6803556866668697\n",
      "tf-idf's MSE, MAE, MAPE, R^2: 0.7309368297659263, 0.6082911613224787, 0.2747411543999557, 0.6710925912356114\n",
      "-------------------------------------------------\n",
      "d2v(val)'s MSE, MAE, MAPE, R^2: 1.1550013230099918, 0.8386253925082852, 0.4037322404867494, 0.48863088680951866\n",
      "d2v's MSE, MAE, MAPE, R^2: 1.143201079913607, 0.8687340089715899, 0.38451185135958355, 0.4855816678283308\n",
      "-------------------------------------------------\n",
      "glove(val)'s MSE, MAE, MAPE, R^2: 0.80130417583294, 0.6754582639355682, 0.30332490410791874, 0.6471754668140799\n",
      "glove's MSE, MAE, MAPE, R^2: 0.8304241889018109, 0.6836580273578113, 0.3061615241549907, 0.6263252075635166\n",
      "-------------------------------------------------\n",
      "bert(val)'s MSE, MAE, MAPE, R^2: 0.6302787140951637, 0.5799574139914321, 0.26335405517730714, 0.7229858912540859\n",
      "bert's MSE, MAE, MAPE, R^2: 0.6419726074329438, 0.5854310516697125, 0.2633064845581584, 0.7111247672714676\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for word_vec in Data.keys():\n",
    "    word_vec_train_rf = 'X_train'\n",
    "    word_vec_test_rf = 'X_test'\n",
    "    \n",
    "    # train model\n",
    "    rf_model.fit(Data[word_vec][word_vec_train_rf], Outdoor_y_train)\n",
    "    \n",
    "    validation(rf_model, Data[word_vec][word_vec_train_rf], Outdoor_y_train, word_vec) # print validation result\n",
    "    \n",
    "    # predict\n",
    "    rf_y_pred_Outdoor = rf_model.predict(Data[word_vec][word_vec_test_rf])\n",
    "    \n",
    "    # evaluate\n",
    "    # print(f\"Evaluating {word_vec}...\")\n",
    "    evaluation(Outdoor_y_test, rf_y_pred_Outdoor, word_vec)\n",
    "    print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_LCLS.fit(Data['LCLS'][word_vec_train_rf], LCLS_y_train)\n",
    "# rf_LCHS.fit(Data['LCHS'][word_vec_train_rf], LCHS_y_train)\n",
    "# rf_HCLS.fit(Data['HCLS'][word_vec_train_rf], HCLS_y_train)\n",
    "# rf_HCHS.fit(Data['HCHS'][word_vec_train_rf], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vec_train_xgb = 'tf'\n",
    "# word_vec_test_xgb = 'tf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf(val)'s MSE, MAE, MAPE, R^2: 0.6649272577944123, 0.6120051486785163, 0.2733206095194375, 0.7068736031506807\n",
      "tf's MSE, MAE, MAPE, R^2: 0.6750885355071242, 0.6075636364986418, 0.26893443876006606, 0.6962232413828425\n",
      "-------------------------------------------------\n",
      "tf-idf(val)'s MSE, MAE, MAPE, R^2: 0.6759974677409398, 0.6117098463372852, 0.2733504669309782, 0.7020163971419845\n",
      "tf-idf's MSE, MAE, MAPE, R^2: 0.6788412345975141, 0.609310547400589, 0.26902270997270117, 0.6945345995147238\n",
      "-------------------------------------------------\n",
      "d2v(val)'s MSE, MAE, MAPE, R^2: 1.0076980448946495, 0.7591408213989453, 0.3439599827099572, 0.5557085797660097\n",
      "d2v's MSE, MAE, MAPE, R^2: 1.0057858541300182, 0.7672987478540124, 0.32739001101791876, 0.5474158564978596\n",
      "-------------------------------------------------\n",
      "glove(val)'s MSE, MAE, MAPE, R^2: 0.7658119843130268, 0.6347019092203274, 0.2764100572632959, 0.6624085304588486\n",
      "glove's MSE, MAE, MAPE, R^2: 0.789336822551551, 0.6430640071469558, 0.2808953611322641, 0.6448137261999969\n",
      "-------------------------------------------------\n",
      "bert(val)'s MSE, MAE, MAPE, R^2: 0.6304042793248328, 0.5582948312280344, 0.24326644466017405, 0.7221169802596729\n",
      "bert's MSE, MAE, MAPE, R^2: 0.6290483937975377, 0.5568936681624024, 0.24228204336328274, 0.7169404129525042\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for word_vec in Data.keys():\n",
    "    word_vec_train_xgb = 'X_train'\n",
    "    word_vec_test_xgb = 'X_test'\n",
    "    \n",
    "    # train model\n",
    "    xgb_model.fit(Data[word_vec][word_vec_train_xgb], Outdoor_y_train)\n",
    "    \n",
    "    validation(xgb_model, Data[word_vec][word_vec_train_xgb], Outdoor_y_train, word_vec) # print validation result\n",
    "    \n",
    "    # predict\n",
    "    xgb_y_pred_Outdoor = xgb_model.predict(Data[word_vec][word_vec_test_xgb])\n",
    "    \n",
    "    # evaluate\n",
    "    # print(f\"Evaluating {word_vec}...\")\n",
    "    evaluation(Outdoor_y_test, xgb_y_pred_Outdoor, word_vec)\n",
    "    print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = 'bert'\n",
    "word_vec_train = 'X_train'\n",
    "word_vec_test = 'X_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[word_vec][word_vec_train]\n",
    "# Outdoor_y_train = Data[word_vec]['y_train']\n",
    "subdata_X_test_embedding = Data[word_vec][word_vec_test]\n",
    "# Outdoor_y_test = Data[word_vec]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [13:19<00:00, 16.00s/trial, best loss: 0.4474249707022514]\n",
      "Best hyperparameters: {'activation': 0, 'batch_size': 32.0, 'dropout': 0.3824367252044485, 'epochs': 50.0, 'optimizer': 1, 'units': 256.0, 'units_h': 160.0}\n",
      "Best MSE: 0.4474249707022514\n"
     ]
    }
   ],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def MLP_model_para(X_train_embedding, params):\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(units=int(params['units']), input_dim=X_train_embedding.shape[1], activation=params['activation']))\n",
    "    nn_model.add(Dropout(params['dropout']))\n",
    "    nn_model.add(Dense(units=int(params['units_h']), activation=params['activation']))\n",
    "    nn_model.add(Dropout(params['dropout']))\n",
    "    nn_model.add(Dense(1, activation=params['activation']))  # Use linear for regression \n",
    "    \n",
    "    nn_model.compile(loss='mean_squared_error', optimizer=params['optimizer'])\n",
    "    return nn_model\n",
    "\n",
    "def objective(params):\n",
    "    model = MLP_model_para(subdata_X_train_embedding.to_numpy(), params)\n",
    "\n",
    "    model.fit(subdata_X_train_embedding.to_numpy(), Outdoor_y_train.to_numpy(), epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(subdata_X_test_embedding.to_numpy())\n",
    "    mse = mean_squared_error(Outdoor_y_test, y_pred)\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'units': hp.quniform('units', 32, 256, 32),\n",
    "    'units_h': hp.quniform('units_h', 32, 256, 32),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
    "    'dropout': hp.uniform('dropout', 0, 1),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()  # Create a trials object to track the progress\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Can also access the results and losses from the trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 0,\n",
       " 'batch_size': 32.0,\n",
       " 'dropout': 0.3824367252044485,\n",
       " 'epochs': 50.0,\n",
       " 'optimizer': 1,\n",
       " 'units': 256.0,\n",
       " 'units_h': 160.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "# 'activation': 0, 'batch_size': 32.0, 'dropout': 0.5449242877882671, 'epochs': 50.0, 'optimizer': 2, 'units': 256.0, 'units_h': 96.0\n",
    "activation = 'relu'\n",
    "batch_size = int(best['batch_size'])\n",
    "dropout = best['dropout']\n",
    "epochs = int(best['epochs'])\n",
    "optimizer = 'rmsprop'\n",
    "units = int(best['units'])\n",
    "units_h = int(best['units_h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(X_train_embedding, y_train):\n",
    "  # define model\n",
    "  nn_model = Sequential()\n",
    "  # Input - Layer\n",
    "  nn_model.add(Dense(units=128, input_dim=X_train_embedding.shape[1], activation='relu'))\n",
    "  # Hidden - Layers\n",
    "  nn_model.add(Dropout(0.3))\n",
    "  nn_model.add(Dense(units=64, activation='relu'))\n",
    "  nn_model.add(Dropout(0.3))\n",
    "  # Output- Layer\n",
    "  nn_model.add(Dense(1, activation='relu'))\n",
    "\n",
    "  nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    y = y.to_numpy()  # Convert Series to NumPy array\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = MLP_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Epoch 27: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 36: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 35: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Epoch 29: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 37: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 49: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 25: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 31: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 35: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "bert(val)'s MSE, MAE, MAPE, R^2: 0.43685770909529265, 0.466755505497496, 0.20602951917662593, 0.8072728070384813\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores,  mae_scores, mape_scores,  r2_scores = cross_val_metrics(subdata_X_train_embedding, Outdoor_y_train)\n",
    "\n",
    "print(f\"{word_vec}(val)'s MSE, MAE, MAPE, R^2: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 2.6786 - val_loss: 0.5926\n",
      "Epoch 2/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.8868 - val_loss: 0.6578\n",
      "Epoch 3/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 829us/step - loss: 0.8175 - val_loss: 0.5890\n",
      "Epoch 4/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 0.7206 - val_loss: 0.5441\n",
      "Epoch 5/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.6767 - val_loss: 0.4940\n",
      "Epoch 6/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step - loss: 0.6476 - val_loss: 0.4755\n",
      "Epoch 7/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.5969 - val_loss: 0.5436\n",
      "Epoch 8/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step - loss: 0.5989 - val_loss: 0.4713\n",
      "Epoch 9/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841us/step - loss: 0.5786 - val_loss: 0.4617\n",
      "Epoch 10/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step - loss: 0.5609 - val_loss: 0.4523\n",
      "Epoch 11/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.5589 - val_loss: 0.4611\n",
      "Epoch 12/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.5382 - val_loss: 0.4745\n",
      "Epoch 13/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.5313 - val_loss: 0.4780\n",
      "Epoch 14/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 0.5351 - val_loss: 0.4825\n",
      "Epoch 15/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step - loss: 0.5292 - val_loss: 0.4728\n",
      "Epoch 16/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.5070 - val_loss: 0.4631\n",
      "Epoch 17/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step - loss: 0.5060 - val_loss: 0.4560\n",
      "Epoch 18/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845us/step - loss: 0.5086 - val_loss: 0.4600\n",
      "Epoch 19/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 0.5119 - val_loss: 0.4590\n",
      "Epoch 20/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 900us/step - loss: 0.4743 - val_loss: 0.4500\n",
      "Epoch 21/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step - loss: 0.4755 - val_loss: 0.4617\n",
      "Epoch 22/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4855 - val_loss: 0.4573\n",
      "Epoch 23/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4761 - val_loss: 0.4896\n",
      "Epoch 24/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4820 - val_loss: 0.4756\n",
      "Epoch 25/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4717 - val_loss: 0.4671\n",
      "Epoch 26/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.4761 - val_loss: 0.4689\n",
      "Epoch 27/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4728 - val_loss: 0.4507\n",
      "Epoch 28/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4717 - val_loss: 0.4533\n",
      "Epoch 29/50\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4598 - val_loss: 0.5055\n",
      "Epoch 30/50\n",
      "\u001b[1m600/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 0.4593Restoring model weights from the end of the best epoch: 20.\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4593 - val_loss: 0.5044\n",
      "Epoch 30: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x747967f5a750>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "mlp_model = MLP_model(subdata_X_train_embedding, Outdoor_y_train)\n",
    "mlp_model.fit(subdata_X_train_embedding, Outdoor_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "bert's MSE, MAE, MAPE, R^2: 0.4557232295198571, 0.47798836595652844, 0.21223091047791315, 0.7949333780552341\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "mlp_y_pred = mlp_model.predict(subdata_X_test_embedding)\n",
    "mlp_y_pred = mlp_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(Outdoor_y_test, mlp_y_pred, word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = 'bert'\n",
    "word_vec_train = 'X_train'\n",
    "word_vec_test = 'X_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[word_vec][word_vec_train]\n",
    "subdata_X_test_embedding = Data[word_vec][word_vec_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata_X_train_embedding = subdata_X_train_embedding.to_numpy().reshape(subdata_X_train_embedding.shape[0], 1, subdata_X_train_embedding.shape[1])\n",
    "subdata_X_test_embedding = subdata_X_test_embedding.to_numpy().reshape(subdata_X_test_embedding.shape[0], 1, subdata_X_test_embedding.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [24:19<00:00, 29.19s/trial, best loss: 0.44304408451437416]\n",
      "Best hyperparameters: {'batch_size': 32.0, 'dropout': 0.06625586623254895, 'epochs': 100.0, 'optimizer': 0, 'return_sequences': 0, 'units': 256.0}\n",
      "Best MSE: 0.44304408451437416\n"
     ]
    }
   ],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def lstm_model_para(X_train_embedding, params):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=int(params['units']), return_sequences=params['return_sequences'], input_shape=(X_train_embedding.shape[1], X_train_embedding.shape[2])))\n",
    "    lstm_model.add(Dropout(params['dropout']))\n",
    "    lstm_model.add(Dense(units=1, activation='linear'))\n",
    "    lstm_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return lstm_model\n",
    "\n",
    "def objective(params):\n",
    "    lstm_model = lstm_model_para(subdata_X_train_embedding, params)\n",
    "\n",
    "    lstm_model.fit(subdata_X_train_embedding, Outdoor_y_train, epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = lstm_model.predict(subdata_X_test_embedding)\n",
    "    mse = mean_squared_error(Outdoor_y_test, y_pred.flatten())\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'units': hp.quniform('units', 16, 256, 32),\n",
    "    'return_sequences': hp.choice('return_sequences', [True, False]),\n",
    "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Also access the results and losses from the trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32.0,\n",
       " 'dropout': 0.06625586623254895,\n",
       " 'epochs': 100.0,\n",
       " 'optimizer': 0,\n",
       " 'return_sequences': 0,\n",
       " 'units': 256.0}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best['batch_size'])\n",
    "units = int(best['units'])\n",
    "dropout = best['dropout']\n",
    "epochs = int(best['epochs'])\n",
    "optimizer = 'adam'\n",
    "return_sequences = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train_embedding, y_train): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=return_sequences, input_shape=(X_train_embedding.shape[1], X_train_embedding.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    # rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    # corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    # # X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    # y = y.values.ravel()  # Convert Series to NumPy array\n",
    "    y = y.reset_index(drop=True)  # reset y index\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = LSTM_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores, mae_scores, mape_scores, r2_scores = cross_val_metrics(subdata_X_train_embedding, Outdoor_y_train)\n",
    "print(f\"{word_vec}(val)'MSE, MAE, MAPE, CC: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0791 - val_loss: 0.5643\n",
      "Epoch 2/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5559 - val_loss: 0.5675\n",
      "Epoch 3/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5122 - val_loss: 0.5281\n",
      "Epoch 4/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5025 - val_loss: 0.5161\n",
      "Epoch 5/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4735 - val_loss: 0.4993\n",
      "Epoch 6/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4780 - val_loss: 0.5236\n",
      "Epoch 7/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4645 - val_loss: 0.4829\n",
      "Epoch 8/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4667 - val_loss: 0.4803\n",
      "Epoch 9/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4551 - val_loss: 0.4605\n",
      "Epoch 10/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4435 - val_loss: 0.4589\n",
      "Epoch 11/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4450 - val_loss: 0.4549\n",
      "Epoch 12/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4357 - val_loss: 0.4632\n",
      "Epoch 13/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4430 - val_loss: 0.4678\n",
      "Epoch 14/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4372 - val_loss: 0.4503\n",
      "Epoch 15/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4439 - val_loss: 0.4565\n",
      "Epoch 16/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4245 - val_loss: 0.4538\n",
      "Epoch 17/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4154 - val_loss: 0.4489\n",
      "Epoch 18/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4131 - val_loss: 0.4684\n",
      "Epoch 19/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4294 - val_loss: 0.4480\n",
      "Epoch 20/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4246 - val_loss: 0.4508\n",
      "Epoch 21/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4204 - val_loss: 0.4501\n",
      "Epoch 22/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4139 - val_loss: 0.4466\n",
      "Epoch 23/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4135 - val_loss: 0.4476\n",
      "Epoch 24/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4149 - val_loss: 0.4505\n",
      "Epoch 25/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4054 - val_loss: 0.4487\n",
      "Epoch 26/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4005 - val_loss: 0.4516\n",
      "Epoch 27/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4208 - val_loss: 0.4496\n",
      "Epoch 28/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4042 - val_loss: 0.4477\n",
      "Epoch 29/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4013 - val_loss: 0.4514\n",
      "Epoch 30/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4097 - val_loss: 0.4486\n",
      "Epoch 31/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4027 - val_loss: 0.4465\n",
      "Epoch 32/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4095 - val_loss: 0.4460\n",
      "Epoch 33/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3976 - val_loss: 0.4604\n",
      "Epoch 34/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4025 - val_loss: 0.4591\n",
      "Epoch 35/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3840 - val_loss: 0.4549\n",
      "Epoch 36/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3919 - val_loss: 0.4758\n",
      "Epoch 37/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4090 - val_loss: 0.4720\n",
      "Epoch 38/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3966 - val_loss: 0.4466\n",
      "Epoch 39/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3811 - val_loss: 0.4536\n",
      "Epoch 40/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3965 - val_loss: 0.4602\n",
      "Epoch 41/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3778 - val_loss: 0.4703\n",
      "Epoch 42/100\n",
      "\u001b[1m597/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3772Restoring model weights from the end of the best epoch: 32.\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3772 - val_loss: 0.4493\n",
      "Epoch 42: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x74796859de50>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "lstm_model = LSTM_model(subdata_X_train_embedding, Outdoor_y_train)\n",
    "lstm_model.fit(subdata_X_train_embedding, Outdoor_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "bert's MSE, MAE, MAPE, R^2: 0.4469591759249226, 0.4540367859218738, 0.19279801341748193, 0.7988770323366927\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "lstm_y_pred = lstm_model.predict(subdata_X_test_embedding)\n",
    "lstm_y_pred = lstm_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(Outdoor_y_test, lstm_y_pred, word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_cnn = 'bert'\n",
    "word_vec_train_cnn = 'X_train'\n",
    "word_vec_test_cnn = 'X_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[word_vec_cnn][word_vec_train_cnn]\n",
    "# subdata_y_train = Data[sub_dataset_cnn]['y_train']\n",
    "subdata_X_test_embedding = Data[word_vec_cnn][word_vec_test_cnn]\n",
    "# subdata_y_test = Data[sub_dataset_cnn]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 139ms/step\n",
      "\u001b[1m 75/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 679us/step \n",
      "\u001b[1m154/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 656us/step\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 11.                     \n",
      "\n",
      "Epoch 21: early stopping                                                        \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 149ms/step            \n",
      "\u001b[1m 72/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 710us/step    \n",
      "\u001b[1m147/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 692us/step   \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 211ms/step             \n",
      "\u001b[1m 75/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 682us/step     \n",
      "\u001b[1m155/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 653us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 222ms/step             \n",
      "\u001b[1m 44/189\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step       \n",
      "\u001b[1m 89/189\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m133/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m178/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 191ms/step             \n",
      "\u001b[1m 79/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 644us/step     \n",
      "\u001b[1m169/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 597us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 221ms/step             \n",
      "\u001b[1m 78/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 657us/step     \n",
      "\u001b[1m160/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 636us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 13.                      \n",
      "\n",
      "Epoch 23: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 151ms/step             \n",
      "\u001b[1m 78/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 655us/step     \n",
      "\u001b[1m156/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 651us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 204ms/step             \n",
      "\u001b[1m 74/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 692us/step     \n",
      "\u001b[1m156/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 651us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 215ms/step             \n",
      "\u001b[1m 63/189\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 818us/step    \n",
      "\u001b[1m126/189\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 808us/step   \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step     \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 154ms/step            \n",
      "\u001b[1m 71/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 723us/step    \n",
      "\u001b[1m144/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 704us/step   \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step     \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 42.                      \n",
      "\n",
      "Epoch 52: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 217ms/step             \n",
      "\u001b[1m 74/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 687us/step     \n",
      "\u001b[1m150/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 674us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 201ms/step             \n",
      "\u001b[1m 71/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 715us/step     \n",
      "\u001b[1m147/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 686us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 29.                      \n",
      "\n",
      "Epoch 39: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 179ms/step             \n",
      "\u001b[1m 67/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 760us/step     \n",
      "\u001b[1m135/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 751us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 34.                      \n",
      "\n",
      "Epoch 44: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 199ms/step             \n",
      "\u001b[1m 86/189\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 589us/step     \n",
      "\u001b[1m174/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 580us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 208ms/step             \n",
      "\u001b[1m 75/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 677us/step     \n",
      "\u001b[1m159/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 634us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 41.                      \n",
      "\n",
      "Epoch 51: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 214ms/step             \n",
      "\u001b[1m 66/189\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 773us/step     \n",
      "\u001b[1m138/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 734us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 210ms/step             \n",
      "\u001b[1m 44/189\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step       \n",
      "\u001b[1m 74/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m118/189\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m162/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 181ms/step             \n",
      "\u001b[1m 77/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 663us/step     \n",
      "\u001b[1m155/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 654us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 150ms/step             \n",
      "\u001b[1m 58/189\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 880us/step     \n",
      "\u001b[1m117/189\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 868us/step    \n",
      "\u001b[1m176/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 863us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 219ms/step             \n",
      "\u001b[1m 74/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 687us/step     \n",
      "\u001b[1m156/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 649us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 36.                      \n",
      "\n",
      "Epoch 46: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 180ms/step             \n",
      "\u001b[1m 70/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 730us/step     \n",
      "\u001b[1m142/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 716us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 28.                       \n",
      "\n",
      "Epoch 38: early stopping                                                          \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 184ms/step              \n",
      "\u001b[1m 72/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 706us/step      \n",
      "\u001b[1m149/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 679us/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step       \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step       \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 55.                       \n",
      "\n",
      "Epoch 65: early stopping                                                          \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 183ms/step              \n",
      "\u001b[1m 77/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 666us/step      \n",
      "\u001b[1m155/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 654us/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step       \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step       \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 40.                       \n",
      "\n",
      "Epoch 50: early stopping                                                        \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 181ms/step            \n",
      "\u001b[1m 69/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 737us/step    \n",
      "\u001b[1m140/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 723us/step   \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 35.                     \n",
      "\n",
      "Epoch 45: early stopping                                                        \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 166ms/step            \n",
      "\u001b[1m 75/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 677us/step    \n",
      "\u001b[1m155/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 651us/step   \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step     \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 38.                     \n",
      "\n",
      "Epoch 48: early stopping                                                        \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 192ms/step            \n",
      "\u001b[1m 77/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 663us/step    \n",
      "\u001b[1m165/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 613us/step   \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step     \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 48.                      \n",
      "\n",
      "Epoch 58: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 192ms/step             \n",
      "\u001b[1m 79/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 644us/step     \n",
      "\u001b[1m167/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 607us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 28.                      \n",
      "\n",
      "Epoch 38: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 171ms/step             \n",
      "\u001b[1m 75/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 677us/step     \n",
      "\u001b[1m161/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 627us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 56.                      \n",
      "\n",
      "Epoch 66: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 171ms/step             \n",
      "\u001b[1m 72/189\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 709us/step     \n",
      "\u001b[1m150/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 674us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\n",
      "Restoring model weights from the end of the best epoch: 51.                      \n",
      "\n",
      "Epoch 61: early stopping                                                         \n",
      "\n",
      "\u001b[1m  1/189\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 166ms/step             \n",
      "\u001b[1m 81/189\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 629us/step     \n",
      "\u001b[1m169/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 599us/step    \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step      \n",
      "\n",
      "100%|██████████| 30/30 [31:38<00:00, 63.30s/trial, best loss: 0.4510689963278213]\n",
      "Best hyperparameters: {'batch_size': 96.0, 'dense_units': 192.0, 'dropout': 0.6750301027831898, 'epochs': 90.0, 'filters': 96.0, 'filters_1': 192.0, 'kernel_size': 0, 'optimizer': 0, 'pool_size': 2}\n",
      "Best MSE: 0.4510689963278213\n"
     ]
    }
   ],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# fill in missing values with the mean of the column\n",
    "subdata_X_train_embedding.fillna(subdata_X_train_embedding.mean(), inplace=True)\n",
    "# subdata_y_train.fillna(subdata_y_train.mean(), inplace=True)\n",
    "subdata_X_test_embedding.fillna(subdata_X_test_embedding.mean(), inplace=True)\n",
    "# subdata_y_test.fillna(subdata_y_test.mean(), inplace=True)\n",
    "\n",
    "def CNN_model_para(X_train_shape, params):\n",
    "    cnn_model = Sequential()\n",
    "    # 1st Conv1D + MaxPooling1D layer  \n",
    "    cnn_model.add(Conv1D(filters=int(params['filters']), kernel_size=int(params['kernel_size']), activation='relu', padding='same', input_shape=(X_train_shape[1], 1)))\n",
    "    cnn_model.add(Conv1D(filters=int(params['filters_1']), kernel_size=int(params['kernel_size']), activation='relu', padding='same'))\n",
    "    cnn_model.add(MaxPooling1D(int(params['pool_size']), padding='same'))\n",
    "    # Flatten\n",
    "    cnn_model.add(Flatten())\n",
    "    # Fully connected layers\n",
    "    cnn_model.add(Dropout(params['dropout']))\n",
    "    cnn_model.add(Dense(units=int(params['dense_units']), activation='relu'))\n",
    "    # # Output layer for regression\n",
    "    cnn_model.add(Dense(1, activation='linear'))\n",
    "    cnn_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return cnn_model\n",
    "\n",
    "def objective(params):\n",
    "    cnn_model = CNN_model_para(subdata_X_train_embedding.shape, params)\n",
    "    cnn_model.fit(subdata_X_train_embedding, Outdoor_y_train, epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    y_pred = cnn_model.predict(subdata_X_test_embedding)\n",
    "    mse = mean_squared_error(Outdoor_y_test, y_pred)\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'filters': hp.quniform('filters', 32, 256, 32),\n",
    "    'filters_1': hp.quniform('filters_1', 32, 256, 32),\n",
    "    'kernel_size': hp.choice('kernel_size', [7, 9, 11, 13]),\n",
    "    'pool_size': hp.choice('pool_size', [2, 3, 5]),\n",
    "    'dropout': hp.uniform('dropout', 0, 1),\n",
    "    'dense_units': hp.quniform('dense_units', 32, 256, 32),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Can also access the results and losses from th\n",
    "# e trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 96.0,\n",
       " 'dense_units': 192.0,\n",
       " 'dropout': 0.6750301027831898,\n",
       " 'epochs': 90.0,\n",
       " 'filters': 96.0,\n",
       " 'filters_1': 192.0,\n",
       " 'kernel_size': 0,\n",
       " 'optimizer': 0,\n",
       " 'pool_size': 2}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best['batch_size'])\n",
    "dense_units = int(best['dense_units'])\n",
    "dropout = best['dropout']\n",
    "epochs = int(best['epochs'])\n",
    "filters = int(best['filters'])\n",
    "filters_1 = int(best['filters_1'])\n",
    "kernel_size = 7\n",
    "optimizer = 'adam'\n",
    "pool_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(X_train_embedding, y_train):\n",
    "    # define model\n",
    "    cnn_model = Sequential()\n",
    "    \n",
    "    # Conv1D 2 layer + MaxPooling1D layer\n",
    "    cnn_model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=(X_train_embedding.shape[1], 1)))\n",
    "    cnn_model.add(Conv1D(filters=filters_1, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size, padding='same'))\n",
    "    \n",
    "    # 2nd Conv1D + MaxPooling1D layer\n",
    "    # cnn_model.add(Conv1D(filters=int(filters*2), kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    # cnn_model.add(MaxPooling1D(pool_size, padding='same'))\n",
    "    cnn_model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers\n",
    "    cnn_model.add(Dropout(dropout))\n",
    "    cnn_model.add(Dense(units=dense_units, activation='relu'))\n",
    "    \n",
    "    # Output layer for regression\n",
    "    cnn_model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    cnn_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    y = y.to_numpy()  # Convert Series to NumPy array\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = CNN_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 56: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 48: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 56: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 44: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Epoch 59: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 56: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 54: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 54: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 58: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 56: early stopping\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "bert's MSE, MAE, MAPE, R^2: 0.4355222249863697, 0.44993060617999925, 0.18414972752136233, 0.8078799929456274\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores, mae_scores, mape_scores, r2_scores = cross_val_metrics(subdata_X_train_embedding, Outdoor_y_train)\n",
    "print(f\"{word_vec_cnn}'s MSE, MAE, MAPE, R^2: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 3.3768 - val_loss: 0.7040\n",
      "Epoch 2/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7280 - val_loss: 0.6239\n",
      "Epoch 3/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6508 - val_loss: 0.5599\n",
      "Epoch 4/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6307 - val_loss: 0.5452\n",
      "Epoch 5/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6161 - val_loss: 0.5348\n",
      "Epoch 6/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6056 - val_loss: 0.5254\n",
      "Epoch 7/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5935 - val_loss: 0.5432\n",
      "Epoch 8/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6003 - val_loss: 0.5577\n",
      "Epoch 9/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5701 - val_loss: 0.5262\n",
      "Epoch 10/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5733 - val_loss: 0.5128\n",
      "Epoch 11/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5524 - val_loss: 0.5096\n",
      "Epoch 12/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5414 - val_loss: 0.5263\n",
      "Epoch 13/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5368 - val_loss: 0.4932\n",
      "Epoch 14/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5204 - val_loss: 0.5119\n",
      "Epoch 15/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.5074 - val_loss: 0.4872\n",
      "Epoch 16/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4978 - val_loss: 0.4869\n",
      "Epoch 17/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4994 - val_loss: 0.5354\n",
      "Epoch 18/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4786 - val_loss: 0.4914\n",
      "Epoch 19/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4807 - val_loss: 0.4846\n",
      "Epoch 20/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4766 - val_loss: 0.4657\n",
      "Epoch 21/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4677 - val_loss: 0.4769\n",
      "Epoch 22/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4653 - val_loss: 0.4746\n",
      "Epoch 23/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4557 - val_loss: 0.4652\n",
      "Epoch 24/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4542 - val_loss: 0.4744\n",
      "Epoch 25/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4540 - val_loss: 0.5057\n",
      "Epoch 26/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4411 - val_loss: 0.4900\n",
      "Epoch 27/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4468 - val_loss: 0.4938\n",
      "Epoch 28/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4525 - val_loss: 0.5061\n",
      "Epoch 29/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4360 - val_loss: 0.4566\n",
      "Epoch 30/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4252 - val_loss: 0.4612\n",
      "Epoch 31/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4139 - val_loss: 0.4738\n",
      "Epoch 32/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4321 - val_loss: 0.4974\n",
      "Epoch 33/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4198 - val_loss: 0.4829\n",
      "Epoch 34/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4194 - val_loss: 0.5318\n",
      "Epoch 35/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4250 - val_loss: 0.4553\n",
      "Epoch 36/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3972 - val_loss: 0.4512\n",
      "Epoch 37/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4119 - val_loss: 0.4456\n",
      "Epoch 38/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4156 - val_loss: 0.4489\n",
      "Epoch 39/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3934 - val_loss: 0.4464\n",
      "Epoch 40/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4023 - val_loss: 0.4605\n",
      "Epoch 41/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4089 - val_loss: 0.4609\n",
      "Epoch 42/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3885 - val_loss: 0.4603\n",
      "Epoch 43/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3885 - val_loss: 0.4558\n",
      "Epoch 44/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3941 - val_loss: 0.4491\n",
      "Epoch 45/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3919 - val_loss: 0.4631\n",
      "Epoch 46/90\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3736 - val_loss: 0.4491\n",
      "Epoch 47/90\n",
      "\u001b[1m199/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3769Restoring model weights from the end of the best epoch: 37.\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3769 - val_loss: 0.4491\n",
      "Epoch 47: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x747969a177d0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "cnn_model = CNN_model(subdata_X_train_embedding, Outdoor_y_train)\n",
    "cnn_model.fit(subdata_X_train_embedding, Outdoor_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step\n",
      "bert's MSE, MAE, MAPE, R^2: 0.4587740738565093, 0.4563819544520127, 0.19422593058036958, 0.793560557225241\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "cnn_y_pred = cnn_model.predict(subdata_X_test_embedding)\n",
    "cnn_y_pred = cnn_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(Outdoor_y_test, cnn_y_pred, word_vec_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
